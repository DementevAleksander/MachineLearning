{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1W8R8WgZceEk"
   },
   "source": [
    "# Character-Level LSTM\n",
    "\n",
    "Мы обучим модель на тексте книги с народными сказками, после чего попробуем генерировать новый текст.\n",
    "\n",
    "**Модель сможет генерировать новый текст на основе текста из народных сказок!**\n",
    "\n",
    "Общая архитектура RNN:\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/charseq.jpeg?raw=1\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sqUOE2flceEl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wHfCDyzceEl"
   },
   "source": [
    "## Загрузим данные\n",
    "\n",
    "Загрузим текстовый файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "b34kfqIOceEl"
   },
   "outputs": [],
   "source": [
    "with open('skazki.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jp1Ljc4mceEl"
   },
   "source": [
    "Посмотрим первые 100 символов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7VctmLQfceEl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'КОТ В САПОГАХ\\n\\nБыло у мельника три сына, и оставил он им, умирая, всего только мельницу, осла и кота. \\n\\nБратья поделили между собой отцовское добро без нотариуса и судьи, которые бы живо проглотили вс'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iC21bopceEl"
   },
   "source": [
    "### Токенизация\n",
    "\n",
    "Выполним преобразование текста в числовое представление.\n",
    "</br>В ячейках ниже создадим два **словаря** для преобразования символов в целые числа и обратно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tYVlmnxLceEl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('г', 'ц', 'И', 'з', '?', ' ', '\\n', 'ъ', 'Н', 'э', ',', 'п', 'у', 'О', 'е', 'т', 'Р', '2', '4', 'М', '!', 'с', 'я', 'Е', 'а', 'С', 'П', 'Ф', '\\xa0', 'Э', 'В', 'к', '–', 'Ь', 'Ч', '(', ';', 'Ш', 'К', ')', '0', 'ф', 'ч', '.', '\\t', 'Щ', 'У', 'Ы', '9', 'Ю', 'Ж', 'ж', 'ш', 'Х', 'и', 'Л', 'Т', '1', '8', 'н', 'Б', 'о', 'Й', 'р', 'д', '\"', 'ь', 'З', 'Я', 'ы', 'Г', 'ю', 'б', 'Ц', 'л', 'Д', 'м', 'щ', 'А', '*', '/', 'х', '5', 'й', '-', '3', ':', 'в')\n"
     ]
    }
   ],
   "source": [
    "chars = tuple(set(text))\n",
    "print(chars)\n",
    "\n",
    "# set(text) собирает уникальные символы (буквы, пробелы, знаки препинания и т. д.) из текста.\n",
    "# tuple(set(text)) превращает множество в кортеж (упорядоченная структура)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'г', 1: 'ц', 2: 'И', 3: 'з', 4: '?', 5: ' ', 6: '\\n', 7: 'ъ', 8: 'Н', 9: 'э', 10: ',', 11: 'п', 12: 'у', 13: 'О', 14: 'е', 15: 'т', 16: 'Р', 17: '2', 18: '4', 19: 'М', 20: '!', 21: 'с', 22: 'я', 23: 'Е', 24: 'а', 25: 'С', 26: 'П', 27: 'Ф', 28: '\\xa0', 29: 'Э', 30: 'В', 31: 'к', 32: '–', 33: 'Ь', 34: 'Ч', 35: '(', 36: ';', 37: 'Ш', 38: 'К', 39: ')', 40: '0', 41: 'ф', 42: 'ч', 43: '.', 44: '\\t', 45: 'Щ', 46: 'У', 47: 'Ы', 48: '9', 49: 'Ю', 50: 'Ж', 51: 'ж', 52: 'ш', 53: 'Х', 54: 'и', 55: 'Л', 56: 'Т', 57: '1', 58: '8', 59: 'н', 60: 'Б', 61: 'о', 62: 'Й', 63: 'р', 64: 'д', 65: '\"', 66: 'ь', 67: 'З', 68: 'Я', 69: 'ы', 70: 'Г', 71: 'ю', 72: 'б', 73: 'Ц', 74: 'л', 75: 'Д', 76: 'м', 77: 'щ', 78: 'А', 79: '*', 80: '/', 81: 'х', 82: '5', 83: 'й', 84: '-', 85: '3', 86: ':', 87: 'в'}\n"
     ]
    }
   ],
   "source": [
    "int2char = dict(enumerate(chars))\n",
    "print(int2char)\n",
    "\n",
    "# enumerate(chars) пронумеровывает каждый символ.\n",
    "# char2int: сопоставляет символ с числом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'г': 0, 'ц': 1, 'И': 2, 'з': 3, '?': 4, ' ': 5, '\\n': 6, 'ъ': 7, 'Н': 8, 'э': 9, ',': 10, 'п': 11, 'у': 12, 'О': 13, 'е': 14, 'т': 15, 'Р': 16, '2': 17, '4': 18, 'М': 19, '!': 20, 'с': 21, 'я': 22, 'Е': 23, 'а': 24, 'С': 25, 'П': 26, 'Ф': 27, '\\xa0': 28, 'Э': 29, 'В': 30, 'к': 31, '–': 32, 'Ь': 33, 'Ч': 34, '(': 35, ';': 36, 'Ш': 37, 'К': 38, ')': 39, '0': 40, 'ф': 41, 'ч': 42, '.': 43, '\\t': 44, 'Щ': 45, 'У': 46, 'Ы': 47, '9': 48, 'Ю': 49, 'Ж': 50, 'ж': 51, 'ш': 52, 'Х': 53, 'и': 54, 'Л': 55, 'Т': 56, '1': 57, '8': 58, 'н': 59, 'Б': 60, 'о': 61, 'Й': 62, 'р': 63, 'д': 64, '\"': 65, 'ь': 66, 'З': 67, 'Я': 68, 'ы': 69, 'Г': 70, 'ю': 71, 'б': 72, 'Ц': 73, 'л': 74, 'Д': 75, 'м': 76, 'щ': 77, 'А': 78, '*': 79, '/': 80, 'х': 81, '5': 82, 'й': 83, '-': 84, '3': 85, ':': 86, 'в': 87}\n"
     ]
    }
   ],
   "source": [
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "print(char2int)\n",
    "\n",
    "# Обратный словарь: { 'a': 0, 'b': 1, ' ': 2, ... }."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38, 13, 56, ..., 43,  6,  6])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "encoded\n",
    "\n",
    "# Каждый символ из текста заменяется на его числовое представление, используя словарь char2int.\n",
    "# Результат сохраняется в массиве encoded (числовая версия текста)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJIzwzSwceEl"
   },
   "source": [
    "Посмотрим как символы закодировались целыми числами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WK1MYr_9ceEl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38, 13, 56,  5, 30,  5, 25, 78, 26, 13, 70, 78, 53,  6,  6, 60, 69,\n",
       "       74, 61,  5, 12,  5, 76, 14, 74, 66, 59, 54, 31, 24,  5, 15, 63, 54,\n",
       "        5, 21, 69, 59, 24, 10,  5, 54,  5, 61, 21, 15, 24, 87, 54, 74,  5,\n",
       "       61, 59,  5, 54, 76, 10,  5, 12, 76, 54, 63, 24, 22, 10,  5, 87, 21,\n",
       "       14,  0, 61,  5, 15, 61, 74, 66, 31, 61,  5, 76, 14, 74, 66, 59, 54,\n",
       "        1, 12, 10,  5, 61, 21, 74, 24,  5, 54,  5, 31, 61, 15, 24])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azltQy-gceEl"
   },
   "source": [
    "## Предобработка данных\n",
    "\n",
    "Char-RNN ожидает **one-hot encoded** входа, что означает, что каждый символ преобразуется в целое число (через созданный словарь), а затем преобразуется в вектор-столбец, где только соответствующий ему целочисленный индекс будет иметь значение 1, а остальная часть вектора будет заполнена нулями. Давайте создадим для этого функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OnahALhiceEl"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "L3lTdLKfceEl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# check that the function works as expected\n",
    "test_seq = np.array(encoded[:2])\n",
    "one_hot = one_hot_encode(test_seq, 87)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YyL91CuceEl"
   },
   "source": [
    "## Создаем mini-batch'и\n",
    "\n",
    "\n",
    "Создадим мини-батчи для обучения. На простом примере они будут выглядеть так:\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/sequence_batching@1x.png?raw=1\" width=500px>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2ECftYejnvpx"
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Функция генерирует батчи из массива arr (обычно закодированный текст),\n",
    "       где каждый батч состоит из входных данных (x) и соответствующих целевых значений (y).\n",
    "       Она разделяет данные на части фиксированного размера (batch_size x seq_length), подходящие для обучения.\n",
    "\n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: входной массив данных\n",
    "       batch_size: размер батча: количество последовательностей в одном пакете\n",
    "       seq_length: длина каждой последовательности внутри батча, количество символов на последовательность\n",
    "    '''\n",
    "\n",
    "    batch_size_total = batch_size * seq_length # Общее количество элементов в одном батче\n",
    "    n_batches = len(arr)//batch_size_total # Вычисляется, сколько полных батчей можно создать из массива. Дробные батчи (остаток) отбрасываются.\n",
    "\n",
    "    arr = arr[:n_batches * batch_size_total] # Массив обрезается, чтобы его длина делилась нацело на batch_size_total. Это устраняет остаточные данные, которые не вписываются в последний батч.\n",
    "    arr = arr.reshape((batch_size, -1)) # Массив преобразуется в матрицу размером (batch_size, num_steps_per_batch), где строки представляют отдельные последовательности для каждого батча.\n",
    "\n",
    "    for n in range(0, arr.shape[1], seq_length): # Проходим по массиву по seq_length, создавая части данных.\n",
    "        x = arr[:, n:n+seq_length] # Формируется входной батч x, где каждая строка содержит seq_length символов.\n",
    "        y = np.zeros_like(x) # Создаётся массив y, который будет содержать целевые значения для x.\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length] # Все символы в y (кроме последнего) сдвигаются на один шаг вперёд относительно x. Последний символ берётся из следующего сегмента данных arr[:, n+seq_length].\n",
    "        except IndexError: # Исключение (IndexError): если выходит за пределы массива (в последнем батче), последний символ связывается с началом массива, замыкая данные в циклический процесс.\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y # Функция возвращает x (входы) и y (цели) как генератор. Генератор позволяет лениво обрабатывать данные, что экономит память."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9uKOvbqceEl"
   },
   "source": [
    "### Протестируем\n",
    "\n",
    "Теперь создадим несколько наборов данных, и проверим, что происходит, когда мы создаем батчи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "qtKlLXi1ceEl"
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 8, 50) # Количество строк - 8, Длина батча = 50\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Rg5MUTqqceEl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[38 13 56  5 30  5 25 78 26 13 70 78 53  6  6 60 69 74 61  5]\n",
      " [61 74 66 31 12  5 61 59 24  5 72 69 74 24  5 87  5  0 63 22]\n",
      " [52 54 15 66 10  5 31 15 61  5 12 64 61 21 15 61 54 15 21 22]\n",
      " [43  5 29 15 61 15  5 15 24 59 14  1 10  5 21 61  0 74 24 21]\n",
      " [59 14  1  5 61 59 54  5 87 21 11 61 76 59 54 74 54  5 11 63]\n",
      " [ 5  6  6 32 28 70 61 15 61 87 24  5 74 54  5 15 69 10  5 21]\n",
      " [63 31 87 54  5 59 14  5  3 24  3 87 61 59 22 15  5 31  5  3]\n",
      " [ 6 32 28 53 61 63 61 52 61 10  5 84  5 21 31 24  3 24 74  5]]\n",
      "\n",
      "y\n",
      " [[13 56  5 30  5 25 78 26 13 70 78 53  6  6 60 69 74 61  5 12]\n",
      " [74 66 31 12  5 61 59 24  5 72 69 74 24  5 87  5  0 63 22  3]\n",
      " [54 15 66 10  5 31 15 61  5 12 64 61 21 15 61 54 15 21 22  5]\n",
      " [ 5 29 15 61 15  5 15 24 59 14  1 10  5 21 61  0 74 24 21 59]\n",
      " [14  1  5 61 59 54  5 87 21 11 61 76 59 54 74 54  5 11 63 61]\n",
      " [ 6  6 32 28 70 61 15 61 87 24  5 74 54  5 15 69 10  5 21 14]\n",
      " [31 87 54  5 59 14  5  3 24  3 87 61 59 22 15  5 31  5  3 24]\n",
      " [32 28 53 61 63 61 52 61 10  5 84  5 21 31 24  3 24 74  5 64]]\n"
     ]
    }
   ],
   "source": [
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:8, :20])\n",
    "print('\\ny\\n', y[:8, :20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_qHIAEIceEl"
   },
   "source": [
    "Данные сдвинуты на один шаг для `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jouxv0L2ceEl"
   },
   "source": [
    "## Зададим архитектуру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "HlTnDntHceEl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else:\n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Реализуем класс CharRNN</b>, представляющий рекуррентную нейронную сеть (RNN) на основе LSTM (Long Short-Term Memory)\n",
    "для работы с текстовыми данными.\n",
    "Она обучается предсказывать следующий символ в последовательности текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VPq1EA38rBqn"
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "\n",
    "    # Инициализзация параметров модели\n",
    "    def __init__(self,\n",
    "                 tokens, # Список уникальных символов текста (например, алфавит), с которыми работает модель.\n",
    "                 n_hidden=256, # Размер скрытого слоя (количество скрытых нейронов в LSTM). Большее число нейронов позволяет модели выучивать более сложные зависимости.\n",
    "                 n_layers=2, # Количество слоев LSTM (глубина). Более глубокие модели (с большим количеством слоев) позволяют лучше обрабатывать сложные зависимости в данных.\n",
    "                 drop_prob=0.5, # Вероятность отключения нейронов (dropout), используется для предотвращения переобучения.\n",
    "                 lr=0.001 # Скорость обучения. Маленький lr: Сеть обучается медленно. Может найти более точное решение, но есть риск \"застрять\" в локальном минимуме. Большой lr: Сеть обучается быстро, но есть риск \"проскочить\" оптимальное решение.\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "\n",
    "        # Словари преобразуют символы в числа (индексы) и наоборот, чтобы модель могла работать с текстом.\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "\n",
    "        ## Слой LSTM, обрабатывает последовательности символов и сохраняет скрытое состояние между временными шагами.\n",
    "        self.lstm = nn.LSTM(\n",
    "            len(self.chars), # Размер входного слоя равен количеству уникальных символов (вход — закодированный текст).\n",
    "            n_hidden, # Количество нейронов в скрытом слое.\n",
    "            n_layers, # Число LSTM-слоев.\n",
    "            dropout=drop_prob, # Используется между слоями для регуляризации.\n",
    "            batch_first=True # Формат входных данных (batch_size, seq_length, input_size).\n",
    "        )\n",
    "\n",
    "        ## Dropout-слой случайным образом \"выключает\" нейроны с вероятностью drop_prob, предотвращая переобучение.\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        ## Полносвязный (fully connected) слой преобразует выход LSTM в вероятности для каждого символа. Выходной размер равен количеству уникальных символов (len(self.chars)).\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "\n",
    "    # Forward-проход (логика прогон данных через модель). x: Входная последовательность символов, закодированная числами. hidden: Текущее скрытое состояние LSTM.\n",
    "    # Forward-проход задаёт логику всей модели, определяя, как входные данные преобразуются в предсказания.\n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        # Пропускаем входные данные через LSTM слой.\n",
    "        r_output, hidden = self.lstm(x, hidden) # r_output: Результаты LSTM для каждого временного шага.\n",
    "                                                # hidden: Обновлённое скрытое состояние (используется для следующего временного шага или следующего батча).\n",
    "\n",
    "        # Пропускаем выход LSTM через Dropout слой.\n",
    "        out = self.dropout(r_output) # Убирает часть нейронов с вероятностью drop_prob для предотвращения переобучения.\n",
    "\n",
    "        # Преобразует выходные данные в форму, подходящую для полносвязного слоя.\n",
    "        # Убирает временные шаги, чтобы слой мог обрабатывать каждый временной шаг как отдельный пример.\n",
    "        out = out.contiguous().view(-1, self.n_hidden) \n",
    "\n",
    "        # Пропускаем данные через полносвязный слой для предсказания символов.\n",
    "        # Полносвязный слой вычисляет вероятности для каждого символа (на основе скрытого состояния LSTM).\n",
    "        # Количество выходных нейронов равно количеству символов в словаре (len(self.chars)).\n",
    "        out = self.fc(out)\n",
    "            \n",
    "        # Возвращаем предсказания и обновлённое скрытое состояние.\n",
    "        return out, hidden # out: вероятности символов. hidden: обновленное состояние для следующего шага.\n",
    "\n",
    "    ## Инициализация скрытых состояний\n",
    "    # Функция init_hidden служит для создания начального состояния LSTM перед началом работы с последовательностями.\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Создаются тензоры для хранения скрытого состояния (hidden state) и состояния ячейки (cell state) LSTM.\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IrBRlEPceEl"
   },
   "source": [
    "## Обучим модель\n",
    "\n",
    "Во время обучения нужно установить количество эпох, скорость обучения и другие параметры.\n",
    "\n",
    "Используем оптимизатор Adam и кросс-энтропию, считаем loss и, как обычно, выполняем back propagation.\n",
    "\n",
    "Пара подробностей об обучении:\n",
    "> * Во время цикла мы отделяем скрытое состояние от его истории; на этот раз установив его равным новой переменной * tuple *, потому что скрытое состояние LSTM, является кортежем скрытых состояний.\n",
    "* Мы используем [`clip_grad_norm_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html) чтобы избавиться от взрывающихся градиентов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта функция train реализует процесс обучения LSTM-модели.\n",
    "</br>Она содержит шаги, необходимые для тренировки рекуррентной нейронной сети с использованием данных, оценки ошибки и валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "lv8VkRI0ceEl"
   },
   "outputs": [],
   "source": [
    "# Инициализзация параметров\n",
    "def train(\n",
    "    net, # модель, которую нужно обучить.\n",
    "    data, # данные для обучения (последовательности символов или индексов).\n",
    "    epochs=10, # количество эпох (полных проходов через данные).\n",
    "    batch_size=10, # количество последовательностей, обрабатываемых за один шаг.\n",
    "    seq_length=50, # длина каждой последовательности (число временных шагов).\n",
    "    lr=0.001, # скорость обучения (learning rate).\n",
    "    clip=5, # значение для ограничения градиентов, чтобы избежать их взрыва.\n",
    "    val_frac=0.1, # доля данных, выделяемых для валидации.\n",
    "    print_every=10 # частота вывода метрик обучения (каждые print_every шагов).\n",
    "):\n",
    "  \n",
    "    net.train() # Установка модели в режим обучения. Это сообщает PyTorch, что модель будет обучаться (активируются такие механизмы, как Dropout).\n",
    "\n",
    "    # Оптимизатор и функция потерь\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr) # алгоритм оптимизации, который адаптирует скорость обучения для каждого параметра модели.\n",
    "    criterion = nn.CrossEntropyLoss() # измеряет ошибку между предсказаниями модели и истинными значениями, что помогает модели улучшать свои предсказания в задаче классификации.\n",
    "\n",
    "    # Разделение данных на обучающие и валидационные\n",
    "    val_idx = int(len(data)*(1-val_frac)) # val_frac определяет долю данных для валидации (по умолчанию 10%).\n",
    "    data, val_data = data[:val_idx], data[val_idx:] # Данные делятся на обучающие и валидационные.\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "\n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    \n",
    "    # Основной цикл обучения. За каждую эпоху модель проходит через все данные.\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(batch_size) # Скрытое состояние и состояние ячейки инициализируются для каждого батча.\n",
    "\n",
    "        # Данные разбиваются на батчи функцией get_batches. В каждом батче:\n",
    "        # x: входные последовательности.\n",
    "        # y: целевые значения (следующие символы для предсказания).\n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "\n",
    "            # Входные данные кодируются в формате one-hot (матрица, где каждая строка соответствует символу).\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Создание новых переменных для скрытого состояния\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # Сброс градиентов и прогон данных через модель\n",
    "            # Градиенты обнуляются перед обратным распространением.\n",
    "            # Вычисляется выход модели и обновляется скрытое состояние.\n",
    "            net.zero_grad()\n",
    "            output, h = net(inputs, h)\n",
    "\n",
    "            # Вычисление потерь и обратное распространение ошибки\n",
    "            # Функция потерь вычисляет ошибку между предсказаниями модели и целевыми значениями.\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long()) # targets.view(batch_size*seq_length).long() приводит целевые значения к нужной форме.\n",
    "            loss.backward()\n",
    "            \n",
    "            # Ограничение градиентов. Для предотвращения \"взрыва градиентов\" их величины ограничиваются значением clip.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            \n",
    "            opt.step() # Обновление параметров модели\n",
    "\n",
    "            # Валидация на части данных\n",
    "            if counter % print_every == 0:\n",
    "                # Получаем validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                \n",
    "                # Каждые print_every шагов вычисляются метрики на валидационных данных\n",
    "                # Это помогает следить за качеством модели и избегать переобучения.\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "                    # Создание новых переменных для скрытого состояния\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                net.train() # сброс в режим обучения после итерации validation data\n",
    "\n",
    "                # Вывод метрик обучения\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gt0q4KGEceEm"
   },
   "source": [
    "## Определим модель\n",
    "\n",
    "Теперь мы можем создать модель с заданными гиперпараметрами. Определим размеры мини-батчей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ykMcIloEr3G7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(88, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=88, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHy6mECuceEm"
   },
   "source": [
    "### Установим гиперпараметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "soEszcw6Ey6y",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 1.8545... Val Loss: 1.7506\n",
      "Epoch: 1/20... Step: 20... Loss: 1.8384... Val Loss: 1.7345\n",
      "Epoch: 1/20... Step: 30... Loss: 1.8323... Val Loss: 1.7305\n",
      "Epoch: 1/20... Step: 40... Loss: 1.8114... Val Loss: 1.7237\n",
      "Epoch: 1/20... Step: 50... Loss: 1.8359... Val Loss: 1.7188\n",
      "Epoch: 1/20... Step: 60... Loss: 1.8609... Val Loss: 1.7139\n",
      "Epoch: 1/20... Step: 70... Loss: 1.8314... Val Loss: 1.7106\n",
      "Epoch: 1/20... Step: 80... Loss: 1.8186... Val Loss: 1.7066\n",
      "Epoch: 1/20... Step: 90... Loss: 1.8446... Val Loss: 1.7057\n",
      "Epoch: 1/20... Step: 100... Loss: 1.8349... Val Loss: 1.6995\n",
      "Epoch: 2/20... Step: 110... Loss: 1.7720... Val Loss: 1.6968\n",
      "Epoch: 2/20... Step: 120... Loss: 1.8034... Val Loss: 1.6939\n",
      "Epoch: 2/20... Step: 130... Loss: 1.7563... Val Loss: 1.6913\n",
      "Epoch: 2/20... Step: 140... Loss: 1.7577... Val Loss: 1.6872\n",
      "Epoch: 2/20... Step: 150... Loss: 1.7758... Val Loss: 1.6855\n",
      "Epoch: 2/20... Step: 160... Loss: 1.7668... Val Loss: 1.6780\n",
      "Epoch: 2/20... Step: 170... Loss: 1.7821... Val Loss: 1.6759\n",
      "Epoch: 2/20... Step: 180... Loss: 1.7631... Val Loss: 1.6728\n",
      "Epoch: 2/20... Step: 190... Loss: 1.7826... Val Loss: 1.6700\n",
      "Epoch: 2/20... Step: 200... Loss: 1.7937... Val Loss: 1.6668\n",
      "Epoch: 3/20... Step: 210... Loss: 1.7855... Val Loss: 1.6628\n",
      "Epoch: 3/20... Step: 220... Loss: 1.7417... Val Loss: 1.6594\n",
      "Epoch: 3/20... Step: 230... Loss: 1.7120... Val Loss: 1.6570\n",
      "Epoch: 3/20... Step: 240... Loss: 1.7407... Val Loss: 1.6541\n",
      "Epoch: 3/20... Step: 250... Loss: 1.7484... Val Loss: 1.6526\n",
      "Epoch: 3/20... Step: 260... Loss: 1.7685... Val Loss: 1.6488\n",
      "Epoch: 3/20... Step: 270... Loss: 1.7772... Val Loss: 1.6442\n",
      "Epoch: 3/20... Step: 280... Loss: 1.7410... Val Loss: 1.6421\n",
      "Epoch: 3/20... Step: 290... Loss: 1.7277... Val Loss: 1.6387\n",
      "Epoch: 3/20... Step: 300... Loss: 1.7428... Val Loss: 1.6360\n",
      "Epoch: 4/20... Step: 310... Loss: 1.7131... Val Loss: 1.6315\n",
      "Epoch: 4/20... Step: 320... Loss: 1.7159... Val Loss: 1.6288\n",
      "Epoch: 4/20... Step: 330... Loss: 1.7109... Val Loss: 1.6254\n",
      "Epoch: 4/20... Step: 340... Loss: 1.7115... Val Loss: 1.6239\n",
      "Epoch: 4/20... Step: 350... Loss: 1.7007... Val Loss: 1.6208\n",
      "Epoch: 4/20... Step: 360... Loss: 1.6965... Val Loss: 1.6200\n",
      "Epoch: 4/20... Step: 370... Loss: 1.7070... Val Loss: 1.6168\n",
      "Epoch: 4/20... Step: 380... Loss: 1.6807... Val Loss: 1.6124\n",
      "Epoch: 4/20... Step: 390... Loss: 1.7107... Val Loss: 1.6104\n",
      "Epoch: 4/20... Step: 400... Loss: 1.6952... Val Loss: 1.6088\n",
      "Epoch: 5/20... Step: 410... Loss: 1.7017... Val Loss: 1.6082\n",
      "Epoch: 5/20... Step: 420... Loss: 1.7066... Val Loss: 1.6047\n",
      "Epoch: 5/20... Step: 430... Loss: 1.6808... Val Loss: 1.6007\n",
      "Epoch: 5/20... Step: 440... Loss: 1.6439... Val Loss: 1.6008\n",
      "Epoch: 5/20... Step: 450... Loss: 1.6592... Val Loss: 1.5961\n",
      "Epoch: 5/20... Step: 460... Loss: 1.6503... Val Loss: 1.5946\n",
      "Epoch: 5/20... Step: 470... Loss: 1.6836... Val Loss: 1.5906\n",
      "Epoch: 5/20... Step: 480... Loss: 1.6871... Val Loss: 1.5864\n",
      "Epoch: 5/20... Step: 490... Loss: 1.6843... Val Loss: 1.5865\n",
      "Epoch: 5/20... Step: 500... Loss: 1.6737... Val Loss: 1.5834\n",
      "Epoch: 6/20... Step: 510... Loss: 1.6759... Val Loss: 1.5820\n",
      "Epoch: 6/20... Step: 520... Loss: 1.6734... Val Loss: 1.5801\n",
      "Epoch: 6/20... Step: 530... Loss: 1.6451... Val Loss: 1.5756\n",
      "Epoch: 6/20... Step: 540... Loss: 1.6258... Val Loss: 1.5738\n",
      "Epoch: 6/20... Step: 550... Loss: 1.6549... Val Loss: 1.5741\n",
      "Epoch: 6/20... Step: 560... Loss: 1.6603... Val Loss: 1.5707\n",
      "Epoch: 6/20... Step: 570... Loss: 1.6185... Val Loss: 1.5682\n",
      "Epoch: 6/20... Step: 580... Loss: 1.6665... Val Loss: 1.5653\n",
      "Epoch: 6/20... Step: 590... Loss: 1.6564... Val Loss: 1.5638\n",
      "Epoch: 6/20... Step: 600... Loss: 1.6183... Val Loss: 1.5618\n",
      "Epoch: 7/20... Step: 610... Loss: 1.6403... Val Loss: 1.5602\n",
      "Epoch: 7/20... Step: 620... Loss: 1.6358... Val Loss: 1.5579\n",
      "Epoch: 7/20... Step: 630... Loss: 1.6180... Val Loss: 1.5534\n",
      "Epoch: 7/20... Step: 640... Loss: 1.6199... Val Loss: 1.5535\n",
      "Epoch: 7/20... Step: 650... Loss: 1.6061... Val Loss: 1.5520\n",
      "Epoch: 7/20... Step: 660... Loss: 1.6467... Val Loss: 1.5529\n",
      "Epoch: 7/20... Step: 670... Loss: 1.6151... Val Loss: 1.5508\n",
      "Epoch: 7/20... Step: 680... Loss: 1.6133... Val Loss: 1.5466\n",
      "Epoch: 7/20... Step: 690... Loss: 1.5887... Val Loss: 1.5447\n",
      "Epoch: 7/20... Step: 700... Loss: 1.6274... Val Loss: 1.5421\n",
      "Epoch: 8/20... Step: 710... Loss: 1.5947... Val Loss: 1.5400\n",
      "Epoch: 8/20... Step: 720... Loss: 1.5960... Val Loss: 1.5379\n",
      "Epoch: 8/20... Step: 730... Loss: 1.5925... Val Loss: 1.5355\n",
      "Epoch: 8/20... Step: 740... Loss: 1.5997... Val Loss: 1.5344\n",
      "Epoch: 8/20... Step: 750... Loss: 1.6223... Val Loss: 1.5324\n",
      "Epoch: 8/20... Step: 760... Loss: 1.5945... Val Loss: 1.5332\n",
      "Epoch: 8/20... Step: 770... Loss: 1.6110... Val Loss: 1.5325\n",
      "Epoch: 8/20... Step: 780... Loss: 1.6090... Val Loss: 1.5298\n",
      "Epoch: 8/20... Step: 790... Loss: 1.5820... Val Loss: 1.5257\n",
      "Epoch: 8/20... Step: 800... Loss: 1.6031... Val Loss: 1.5261\n",
      "Epoch: 9/20... Step: 810... Loss: 1.5740... Val Loss: 1.5248\n",
      "Epoch: 9/20... Step: 820... Loss: 1.5847... Val Loss: 1.5217\n",
      "Epoch: 9/20... Step: 830... Loss: 1.6235... Val Loss: 1.5214\n",
      "Epoch: 9/20... Step: 840... Loss: 1.5682... Val Loss: 1.5179\n",
      "Epoch: 9/20... Step: 850... Loss: 1.5573... Val Loss: 1.5172\n",
      "Epoch: 9/20... Step: 860... Loss: 1.5757... Val Loss: 1.5186\n",
      "Epoch: 9/20... Step: 870... Loss: 1.5598... Val Loss: 1.5154\n",
      "Epoch: 9/20... Step: 880... Loss: 1.5529... Val Loss: 1.5148\n",
      "Epoch: 9/20... Step: 890... Loss: 1.5869... Val Loss: 1.5117\n",
      "Epoch: 9/20... Step: 900... Loss: 1.5789... Val Loss: 1.5109\n",
      "Epoch: 10/20... Step: 910... Loss: 1.6210... Val Loss: 1.5097\n",
      "Epoch: 10/20... Step: 920... Loss: 1.5408... Val Loss: 1.5085\n",
      "Epoch: 10/20... Step: 930... Loss: 1.5225... Val Loss: 1.5056\n",
      "Epoch: 10/20... Step: 940... Loss: 1.5538... Val Loss: 1.5061\n",
      "Epoch: 10/20... Step: 950... Loss: 1.5856... Val Loss: 1.5027\n",
      "Epoch: 10/20... Step: 960... Loss: 1.5481... Val Loss: 1.5025\n",
      "Epoch: 10/20... Step: 970... Loss: 1.5626... Val Loss: 1.5001\n",
      "Epoch: 10/20... Step: 980... Loss: 1.5617... Val Loss: 1.5010\n",
      "Epoch: 10/20... Step: 990... Loss: 1.5342... Val Loss: 1.4986\n",
      "Epoch: 10/20... Step: 1000... Loss: 1.5611... Val Loss: 1.4970\n",
      "Epoch: 10/20... Step: 1010... Loss: 1.6220... Val Loss: 1.4958\n",
      "Epoch: 11/20... Step: 1020... Loss: 1.5512... Val Loss: 1.4954\n",
      "Epoch: 11/20... Step: 1030... Loss: 1.5480... Val Loss: 1.4932\n",
      "Epoch: 11/20... Step: 1040... Loss: 1.5364... Val Loss: 1.4940\n",
      "Epoch: 11/20... Step: 1050... Loss: 1.5364... Val Loss: 1.4906\n",
      "Epoch: 11/20... Step: 1060... Loss: 1.5627... Val Loss: 1.4919\n",
      "Epoch: 11/20... Step: 1070... Loss: 1.5486... Val Loss: 1.4892\n",
      "Epoch: 11/20... Step: 1080... Loss: 1.5351... Val Loss: 1.4887\n",
      "Epoch: 11/20... Step: 1090... Loss: 1.5299... Val Loss: 1.4865\n",
      "Epoch: 11/20... Step: 1100... Loss: 1.5710... Val Loss: 1.4851\n",
      "Epoch: 11/20... Step: 1110... Loss: 1.5511... Val Loss: 1.4862\n",
      "Epoch: 12/20... Step: 1120... Loss: 1.4994... Val Loss: 1.4847\n",
      "Epoch: 12/20... Step: 1130... Loss: 1.5357... Val Loss: 1.4831\n",
      "Epoch: 12/20... Step: 1140... Loss: 1.4968... Val Loss: 1.4834\n",
      "Epoch: 12/20... Step: 1150... Loss: 1.4964... Val Loss: 1.4803\n",
      "Epoch: 12/20... Step: 1160... Loss: 1.5103... Val Loss: 1.4812\n",
      "Epoch: 12/20... Step: 1170... Loss: 1.4984... Val Loss: 1.4797\n",
      "Epoch: 12/20... Step: 1180... Loss: 1.5177... Val Loss: 1.4774\n",
      "Epoch: 12/20... Step: 1190... Loss: 1.5179... Val Loss: 1.4744\n",
      "Epoch: 12/20... Step: 1200... Loss: 1.5230... Val Loss: 1.4756\n",
      "Epoch: 12/20... Step: 1210... Loss: 1.5409... Val Loss: 1.4764\n",
      "Epoch: 13/20... Step: 1220... Loss: 1.5211... Val Loss: 1.4729\n",
      "Epoch: 13/20... Step: 1230... Loss: 1.5032... Val Loss: 1.4728\n",
      "Epoch: 13/20... Step: 1240... Loss: 1.4730... Val Loss: 1.4722\n",
      "Epoch: 13/20... Step: 1250... Loss: 1.4972... Val Loss: 1.4703\n",
      "Epoch: 13/20... Step: 1260... Loss: 1.5025... Val Loss: 1.4709\n",
      "Epoch: 13/20... Step: 1270... Loss: 1.5178... Val Loss: 1.4703\n",
      "Epoch: 13/20... Step: 1280... Loss: 1.5124... Val Loss: 1.4695\n",
      "Epoch: 13/20... Step: 1290... Loss: 1.5060... Val Loss: 1.4669\n",
      "Epoch: 13/20... Step: 1300... Loss: 1.4972... Val Loss: 1.4664\n",
      "Epoch: 13/20... Step: 1310... Loss: 1.5061... Val Loss: 1.4656\n",
      "Epoch: 14/20... Step: 1320... Loss: 1.4785... Val Loss: 1.4645\n",
      "Epoch: 14/20... Step: 1330... Loss: 1.4896... Val Loss: 1.4627\n",
      "Epoch: 14/20... Step: 1340... Loss: 1.4880... Val Loss: 1.4638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20... Step: 1350... Loss: 1.4888... Val Loss: 1.4621\n",
      "Epoch: 14/20... Step: 1360... Loss: 1.4932... Val Loss: 1.4620\n",
      "Epoch: 14/20... Step: 1370... Loss: 1.4719... Val Loss: 1.4593\n",
      "Epoch: 14/20... Step: 1380... Loss: 1.4758... Val Loss: 1.4616\n",
      "Epoch: 14/20... Step: 1390... Loss: 1.4531... Val Loss: 1.4579\n",
      "Epoch: 14/20... Step: 1400... Loss: 1.4904... Val Loss: 1.4583\n",
      "Epoch: 14/20... Step: 1410... Loss: 1.4819... Val Loss: 1.4601\n",
      "Epoch: 15/20... Step: 1420... Loss: 1.4855... Val Loss: 1.4566\n",
      "Epoch: 15/20... Step: 1430... Loss: 1.5010... Val Loss: 1.4555\n",
      "Epoch: 15/20... Step: 1440... Loss: 1.4677... Val Loss: 1.4544\n",
      "Epoch: 15/20... Step: 1450... Loss: 1.4604... Val Loss: 1.4540\n",
      "Epoch: 15/20... Step: 1460... Loss: 1.4502... Val Loss: 1.4565\n",
      "Epoch: 15/20... Step: 1470... Loss: 1.4539... Val Loss: 1.4537\n",
      "Epoch: 15/20... Step: 1480... Loss: 1.5003... Val Loss: 1.4577\n",
      "Epoch: 15/20... Step: 1490... Loss: 1.4757... Val Loss: 1.4514\n",
      "Epoch: 15/20... Step: 1500... Loss: 1.4966... Val Loss: 1.4507\n",
      "Epoch: 15/20... Step: 1510... Loss: 1.4860... Val Loss: 1.4527\n",
      "Epoch: 16/20... Step: 1520... Loss: 1.4737... Val Loss: 1.4503\n",
      "Epoch: 16/20... Step: 1530... Loss: 1.4757... Val Loss: 1.4495\n",
      "Epoch: 16/20... Step: 1540... Loss: 1.4488... Val Loss: 1.4480\n",
      "Epoch: 16/20... Step: 1550... Loss: 1.4329... Val Loss: 1.4482\n",
      "Epoch: 16/20... Step: 1560... Loss: 1.4695... Val Loss: 1.4481\n",
      "Epoch: 16/20... Step: 1570... Loss: 1.4509... Val Loss: 1.4467\n",
      "Epoch: 16/20... Step: 1580... Loss: 1.4296... Val Loss: 1.4503\n",
      "Epoch: 16/20... Step: 1590... Loss: 1.4702... Val Loss: 1.4443\n",
      "Epoch: 16/20... Step: 1600... Loss: 1.4643... Val Loss: 1.4449\n",
      "Epoch: 16/20... Step: 1610... Loss: 1.4484... Val Loss: 1.4460\n",
      "Epoch: 17/20... Step: 1620... Loss: 1.4700... Val Loss: 1.4449\n",
      "Epoch: 17/20... Step: 1630... Loss: 1.4799... Val Loss: 1.4418\n",
      "Epoch: 17/20... Step: 1640... Loss: 1.4503... Val Loss: 1.4417\n",
      "Epoch: 17/20... Step: 1650... Loss: 1.4486... Val Loss: 1.4415\n",
      "Epoch: 17/20... Step: 1660... Loss: 1.4328... Val Loss: 1.4409\n",
      "Epoch: 17/20... Step: 1670... Loss: 1.4714... Val Loss: 1.4402\n",
      "Epoch: 17/20... Step: 1680... Loss: 1.4437... Val Loss: 1.4408\n",
      "Epoch: 17/20... Step: 1690... Loss: 1.4366... Val Loss: 1.4386\n",
      "Epoch: 17/20... Step: 1700... Loss: 1.4241... Val Loss: 1.4385\n",
      "Epoch: 17/20... Step: 1710... Loss: 1.4701... Val Loss: 1.4403\n",
      "Epoch: 18/20... Step: 1720... Loss: 1.4372... Val Loss: 1.4381\n",
      "Epoch: 18/20... Step: 1730... Loss: 1.4276... Val Loss: 1.4372\n",
      "Epoch: 18/20... Step: 1740... Loss: 1.4400... Val Loss: 1.4365\n",
      "Epoch: 18/20... Step: 1750... Loss: 1.4568... Val Loss: 1.4361\n",
      "Epoch: 18/20... Step: 1760... Loss: 1.4600... Val Loss: 1.4336\n",
      "Epoch: 18/20... Step: 1770... Loss: 1.4432... Val Loss: 1.4366\n",
      "Epoch: 18/20... Step: 1780... Loss: 1.4468... Val Loss: 1.4349\n",
      "Epoch: 18/20... Step: 1790... Loss: 1.4462... Val Loss: 1.4352\n",
      "Epoch: 18/20... Step: 1800... Loss: 1.4275... Val Loss: 1.4331\n",
      "Epoch: 18/20... Step: 1810... Loss: 1.4556... Val Loss: 1.4337\n",
      "Epoch: 19/20... Step: 1820... Loss: 1.4175... Val Loss: 1.4342\n",
      "Epoch: 19/20... Step: 1830... Loss: 1.4375... Val Loss: 1.4330\n",
      "Epoch: 19/20... Step: 1840... Loss: 1.4629... Val Loss: 1.4307\n",
      "Epoch: 19/20... Step: 1850... Loss: 1.4158... Val Loss: 1.4323\n",
      "Epoch: 19/20... Step: 1860... Loss: 1.4162... Val Loss: 1.4312\n",
      "Epoch: 19/20... Step: 1870... Loss: 1.4360... Val Loss: 1.4312\n",
      "Epoch: 19/20... Step: 1880... Loss: 1.4130... Val Loss: 1.4321\n",
      "Epoch: 19/20... Step: 1890... Loss: 1.4103... Val Loss: 1.4346\n",
      "Epoch: 19/20... Step: 1900... Loss: 1.4322... Val Loss: 1.4297\n",
      "Epoch: 19/20... Step: 1910... Loss: 1.4413... Val Loss: 1.4311\n",
      "Epoch: 20/20... Step: 1920... Loss: 1.4899... Val Loss: 1.4285\n",
      "Epoch: 20/20... Step: 1930... Loss: 1.3993... Val Loss: 1.4308\n",
      "Epoch: 20/20... Step: 1940... Loss: 1.3908... Val Loss: 1.4305\n",
      "Epoch: 20/20... Step: 1950... Loss: 1.4137... Val Loss: 1.4285\n",
      "Epoch: 20/20... Step: 1960... Loss: 1.4357... Val Loss: 1.4271\n",
      "Epoch: 20/20... Step: 1970... Loss: 1.4191... Val Loss: 1.4281\n",
      "Epoch: 20/20... Step: 1980... Loss: 1.4232... Val Loss: 1.4271\n",
      "Epoch: 20/20... Step: 1990... Loss: 1.4174... Val Loss: 1.4293\n",
      "Epoch: 20/20... Step: 2000... Loss: 1.3905... Val Loss: 1.4262\n",
      "Epoch: 20/20... Step: 2010... Loss: 1.4267... Val Loss: 1.4260\n",
      "Epoch: 20/20... Step: 2020... Loss: 1.4904... Val Loss: 1.4236\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfZxvNoDceEm"
   },
   "source": [
    "## Checkpoint\n",
    "\n",
    "После обучения сохраним модель, чтобы можно было загрузить ее позже. Здесь сохраняются параметры, необходимые для создания той же архитектуры, гиперпараметры скрытого слоя и токены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "q6RXl5VAceEm"
   },
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn_x_epoch_ru.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2sJhx5iceEm"
   },
   "source": [
    "---\n",
    "## Делаем предсказания\n",
    "\n",
    "Теперь, когда мы обучили модель, сделаем предсказание следующих символов! Для предсказания мы передаем последний символ, и сеть предсказывает следующий символ, который мы потом передаем снова на вхол и получаем еще один предсказанный символ и так далее...\n",
    "\n",
    "Наши прогнозы основаны на категориальном распределении вероятностей по всем возможным символам. Мы можем ограничить число символов, чтобы сделать получаемый предсказанный текст более разумным, рассматривая только некоторые наиболее вероятные символы $K$. Это не позволит сети выдавать нам совершенно абсурдные прогнозы, а также позволит внести некоторый шум и случайность в выбранный текст. Узнать больше [можно здесь](https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "QEIRW_B2ceEm"
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "\n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "\n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "\n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "\n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "\n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OG38j3gQceEm"
   },
   "source": [
    "### Priming и генерирование текста\n",
    "\n",
    "Нужно задать скрытое состояние, чтобы сеть не генерировала произвольные символы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "P9vpB5gRceEm"
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='Король', top_k=None):\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval() # eval mode\n",
    "\n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "\n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "BqmFA9eEceEm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Золотой. \n",
      "\n",
      "И король осла в сад положил к стола и плачу, что не положили его на королевской купочке, платье и веснились и стали принца сверкать на белых соной, поклинывались на пострую своего прочь, и в одной комнате подумала стоят крыльями. Прошел королевна и воскилась на плика. \n",
      "\n",
      "– Как же мой слова перескажите, когда он не поведет на собой принцессе! - возразили платья. - Но откуда то смерть спасеби тебе серебряное столковые слугу. \n",
      "\n",
      "Вот как только они очно сказала: \n",
      "\n",
      "– Не знаю что! Пока не возьму на свете! \n",
      "\n",
      "Принц отпустился в облакой колонец, полно ответил: \n",
      "\n",
      "– Нет, не было слезом, поже нам проводить свою другой колени. \n",
      "\n",
      "И тот королевич сказал: \n",
      "\n",
      "– Как бы твое нужное было послышно подоши в станику? Но я стала на ней самому стольную дверь все, что приданое войско. Ты посмотрел на него не помогу, но нам наколеть проснулся! \n",
      "\n",
      "– Ну поставил мне следуют королевскай мышь и волшебницы, - ответил середин и поставил на королевский дворец. \n",
      "\n",
      "На другой день она сказала: \n",
      "\n",
      "– Принесли, тебя были, так \n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Золото', top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "942mjdQHceEm"
   },
   "source": [
    "## Loading a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Xt9ldUuSceEm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
    "with open('rnn_x_epoch_ru.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "\n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Ut6R3zDcceEm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Золотой Золотой короля во сне в старой комнате на две невидиные два самого совета. \n",
      "\n",
      "Но сказал ему, что она в семь дом поставила в прослый колдуньи, а прявлятся несколько мир. Они все белых детем с какому не привели несколько дней и в сердце ведел сважить. \n",
      "\n",
      "– Не скажи то, что все это не бежать, привысти мне служить перед ними солатье стороного... \n",
      "\n",
      "– Пусть она стала тебе, - сказала она. - Так он покупайте твоей жизни! \n",
      "\n",
      "Перед ним на дво слова подумала с ним по сторонам, когда он пришлась спасить на колени. \n",
      "\n",
      "Поднял сестра, показали все в камни и обрадалась превратиться. Пока отец ного в стороне он увидел на своих статьях, и она стала насметно вороном слуги и подняла королевну. \n",
      "\n",
      "– Когда же мне быть соберальный дворец и положился? - ответил колокольки, покрытые стол. \n",
      "\n",
      "– Не подуму все, что страшно быть нет навеста. \n",
      "\n",
      "– А как же мы не пойдем в моей жене? \n",
      "\n",
      "– Пришлось меня, которой спраслало ему, чтобы наста надежда с ней называться. Надо того он начили принц. \n",
      "\n",
      "Великан взял великан с корылей, обрадовался они в колдунье и выпел великан и согласился с нею. \n",
      "\n",
      "Как проснулся и отослил об этом, как великан принес. \n",
      "\n",
      "– Поспори то, что ты, старая делает себя немного подальше? - вскричали они: - А то вы не выдолни тебе принца, а посмотрем в столенное прекрасное поляне. Пойдем остальным своему дочь волшебный принцессы, и я стану тайный костяник с каждой дочь. \n",
      "\n",
      "Принцесса послала его в подземелье по дому. \n",
      "\n",
      "– Ах, можит она, - говорит ему медведь. - Я не могу подумать сегобня и сокол с ним и подумать соледники, которые выпросало его дала с таким своей дочери и повела ей камень принцессы. \n",
      "\n",
      "– Ах вы случалось, принеси! - воскликнула она, - к соламому свету! \n",
      "\n",
      "И стали он свадьбу и в скольпо днем сверхала. Пасть просножели по своему своему королю и отправил свою дочь с кусочком и привазила свой волный палец. Принц воротился одно девушки. Посмотрел, когда она села на свое стерого свою дочь кровата и сказала: \n",
      "\n",
      "– Скажи, что ты спокойно будешь! \n",
      "\n",
      "– Нет, вод туда, который сказывай, - ответил п\n"
     ]
    }
   ],
   "source": [
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"Золото\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
