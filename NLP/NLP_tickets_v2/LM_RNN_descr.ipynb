{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1W8R8WgZceEk"
   },
   "source": [
    "# NLP Tickets\n",
    "\n",
    "Fine-Tuning модели для конкретной задачи.\n",
    "</br>Взять предобученную модель (например, BERT, GPT или T5) и дообучить её на небольшом наборе данных для специализированной задачи:\n",
    "</br>Классификация пользовательских запросов (например, техподдержка: инцидент, запрос и т.д.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Подготовка данных. Перед тем как начать fine-tuning, нужно подготовить данные:\n",
    "* Разделить тексты на токены.\n",
    "* Преобразовать метки классов в числовые значения.\n",
    "* Разбить данные на обучающую и тестовую выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sqUOE2flceEl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer # автоматически загружает токенизатор для конкретной модели\n",
    "# transformers - библиотека Hugging Face, которая предоставляет инструменты для работы с предобученными моделями обработки естественного языка (NLP)\n",
    "import torch # используется для работы с тензорами (многомерными массивами) и построения нейронных сетей\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим устройство: если доступен GPU, используем его, иначе — CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Тип</th>\n",
       "      <th>Описание</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ошибка/Проблема</td>\n",
       "      <td>При открытии Word появляется сообщение: \"Не уд...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ошибка/Проблема</td>\n",
       "      <td>Excel аварийно закрывается при попытке вставит...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ошибка/Проблема</td>\n",
       "      <td>При сохранении документа в Word вылетает с оши...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ошибка/Проблема</td>\n",
       "      <td>Outlook выдает ошибку \"Не удается открыть окно...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ошибка/Проблема</td>\n",
       "      <td>PowerPoint вылетает при добавлении видеофайла ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>Оборудование</td>\n",
       "      <td>Прошу заменить разветвитель USB-портов - неста...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>Оборудование</td>\n",
       "      <td>Требуется кабель KVM для управления нескольким...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>Оборудование</td>\n",
       "      <td>Необходимо получить адаптер для подключения ст...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>Оборудование</td>\n",
       "      <td>Прошу выдать удлинитель питания с фильтром пом...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>Оборудование</td>\n",
       "      <td>Нужен комплект патч-кордов разной длины для ре...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Тип                                           Описание\n",
       "0    Ошибка/Проблема  При открытии Word появляется сообщение: \"Не уд...\n",
       "1    Ошибка/Проблема  Excel аварийно закрывается при попытке вставит...\n",
       "2    Ошибка/Проблема  При сохранении документа в Word вылетает с оши...\n",
       "3    Ошибка/Проблема  Outlook выдает ошибку \"Не удается открыть окно...\n",
       "4    Ошибка/Проблема  PowerPoint вылетает при добавлении видеофайла ...\n",
       "..               ...                                                ...\n",
       "795     Оборудование  Прошу заменить разветвитель USB-портов - неста...\n",
       "796     Оборудование  Требуется кабель KVM для управления нескольким...\n",
       "797     Оборудование  Необходимо получить адаптер для подключения ст...\n",
       "798     Оборудование  Прошу выдать удлинитель питания с фильтром пом...\n",
       "799     Оборудование  Нужен комплект патч-кордов разной длины для ре...\n",
       "\n",
       "[800 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка данных из Excel файла\n",
    "data = pd.read_excel(\"dataset-tickets_ru.xlsx\")\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование меток в числовые значения\n",
    "label_map = {label: idx for idx, label in enumerate(df[\"Тип\"].unique())}\n",
    "df[\"label\"] = df[\"Тип\"].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"Тип\"].unique()\n",
    "# Результат: ['Ошибка/Проблема', 'Консультации', 'Предложения по улучшению', 'Оборудование']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция enumerate() добавляет порядковый номер (индекс) к каждому элементу в итерируемом объекте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(enumerate(['Ошибка/Проблема', 'Консультации']))\n",
    "# Результат: [(0, 'Ошибка/Проблема'), (1, 'Консультации')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что делает генератор словаря {label: idx for idx, label in ...}?\n",
    "\n",
    "Генератор словаря создаёт пары ключ-значение, где:\n",
    "* Ключ (label) — это уникальная метка класса (например, \"Ошибка/Проблема\"),\n",
    "* Значение (idx) — это числовой индекс этой метки (например, 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {label: idx for idx, label in enumerate(['Ошибка/Проблема', 'Консультации'])}\n",
    "# Результат: {'Ошибка/Проблема': 0, 'Консультации': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ошибка/Проблема': 0,\n",
       " 'Консультации': 1,\n",
       " 'Предложения по улучшению': 2,\n",
       " 'Оборудование': 3}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Тип</th>\n",
       "      <th>Описание</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ошибка/Проблема</td>\n",
       "      <td>При открытии Word появляется сообщение: \"Не уд...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ошибка/Проблема</td>\n",
       "      <td>Excel аварийно закрывается при попытке вставит...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ошибка/Проблема</td>\n",
       "      <td>При сохранении документа в Word вылетает с оши...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ошибка/Проблема</td>\n",
       "      <td>Outlook выдает ошибку \"Не удается открыть окно...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ошибка/Проблема</td>\n",
       "      <td>PowerPoint вылетает при добавлении видеофайла ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>Оборудование</td>\n",
       "      <td>Прошу заменить разветвитель USB-портов - неста...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>Оборудование</td>\n",
       "      <td>Требуется кабель KVM для управления нескольким...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>Оборудование</td>\n",
       "      <td>Необходимо получить адаптер для подключения ст...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>Оборудование</td>\n",
       "      <td>Прошу выдать удлинитель питания с фильтром пом...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>Оборудование</td>\n",
       "      <td>Нужен комплект патч-кордов разной длины для ре...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Тип                                           Описание  label\n",
       "0    Ошибка/Проблема  При открытии Word появляется сообщение: \"Не уд...      0\n",
       "1    Ошибка/Проблема  Excel аварийно закрывается при попытке вставит...      0\n",
       "2    Ошибка/Проблема  При сохранении документа в Word вылетает с оши...      0\n",
       "3    Ошибка/Проблема  Outlook выдает ошибку \"Не удается открыть окно...      0\n",
       "4    Ошибка/Проблема  PowerPoint вылетает при добавлении видеофайла ...      0\n",
       "..               ...                                                ...    ...\n",
       "795     Оборудование  Прошу заменить разветвитель USB-портов - неста...      3\n",
       "796     Оборудование  Требуется кабель KVM для управления нескольким...      3\n",
       "797     Оборудование  Необходимо получить адаптер для подключения ст...      3\n",
       "798     Оборудование  Прошу выдать удлинитель питания с фильтром пом...      3\n",
       "799     Оборудование  Нужен комплект патч-кордов разной длины для ре...      3\n",
       "\n",
       "[800 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделяем признаки (X) и целевую переменную (y)\n",
    "X = df[[\"Описание\",]]  # Все признаки\n",
    "y = df[\"label\"]  # Целевая переменная\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Сброс индексов для y_train и y_test\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры обучающей выборки (X_train): (640, 1)\n",
      "Размеры тестовой выборки (X_test): (160, 1)\n",
      "Размеры меток обучающей выборки (y_train): (640,)\n",
      "Размеры меток тестовой выборки (y_test): (160,)\n"
     ]
    }
   ],
   "source": [
    "# Проверка размеров\n",
    "print(\"Размеры обучающей выборки (X_train):\", X_train.shape)\n",
    "print(\"Размеры тестовой выборки (X_test):\", X_test.shape)\n",
    "print(\"Размеры меток обучающей выборки (y_train):\", y_train.shape)\n",
    "print(\"Размеры меток тестовой выборки (y_test):\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка токенизатора\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruBert-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация: Тексты преобразуются в токены с помощью токенизатора BERT. Мы ограничиваем длину последовательности до 128 токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токенизация данных\n",
    "# Токенизация текстовых данных\n",
    "def tokenize_data(texts):\n",
    "    return tokenizer(\n",
    "        list(texts), \n",
    "        padding=True, # Дополняет последовательности до одной длины.\n",
    "        truncation=True, # Обрезает последовательности, если они превышают max_length.\n",
    "        max_length=128, # Ограничивает длину последовательности до 128 токенов.\n",
    "        return_tensors=\"pt\" # Возвращает данные в формате PyTorch.\n",
    "    )\n",
    "\n",
    "# Токенизация обучающих и тестовых данных\n",
    "X_train_tokenized = tokenize_data(X_train[\"Описание\"])\n",
    "X_test_tokenized = tokenize_data(X_test[\"Описание\"])\n",
    "\n",
    "# Преобразование других признаков (например, One-Hot Encoding)\n",
    "# X_train_other = pd.get_dummies(X_train[[\"Приоритет\", \"Категория\"]])\n",
    "# X_test_other = pd.get_dummies(X_test[[\"Приоритет\", \"Категория\"]])\n",
    "\n",
    "# Теперь у вас есть:\n",
    "# - X_train_tokenized: Токенизированные тексты для обучающей выборки.\n",
    "# - X_train_other: Числовые признаки для обучающей выборки.\n",
    "# - y_train: Метки для обучающей выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токенизированные данные для 'Описание':\n",
      "{'input_ids': tensor([[  101,  1232,   797,  ...,     0,     0,     0],\n",
      "        [  101, 10858, 19630,  ...,     0,     0,     0],\n",
      "        [  101,  1153,  2775,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,   785,  5761,  ...,     0,     0,     0],\n",
      "        [  101, 24085, 12628,  ...,     0,     0,     0],\n",
      "        [  101, 67479, 76733,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# Пример использования токенизированных данных\n",
    "print(\"Токенизированные данные для 'Описание':\")\n",
    "print(X_train_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Загрузка предобученной модели\n",
    "\n",
    "Мы загружаем предобученную модель ruBert-large и добавляем новый слой для классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruBert-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(120138, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Загрузка предобученной модели\n",
    "num_labels = len(df[\"Тип\"].unique())  # Количество классов\n",
    "model = BertForSequenceClassification.from_pretrained(\"ai-forever/ruBert-large\", num_labels=num_labels)\n",
    "\n",
    "# Перемещение модели на GPU (если доступно)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объяснение:\n",
    "* Загрузка модели: Мы используем BertForSequenceClassification, которая уже содержит выходной слой для классификации.\n",
    "* Настройка количества классов: Указываем количество классов (num_labels) в зависимости от уникальных меток в данных.\n",
    "* Устройство: Если доступна GPU, модель перемещается на неё для ускорения обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Создание DataLoader\n",
    "\n",
    "Чтобы обучать модель эффективно, мы создаем объекты DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings  # Токенизированные данные для \"Описания\"\n",
    "        self.labels = labels  # Метки классов\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)  # Возвращает количество примеров в датасете.\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}  # Получаем токенизированные данные для \"Описания\"\n",
    "        item[\"labels\"] = self.labels[idx]  # Добавляем метку класса\n",
    "        return item\n",
    "\n",
    "# Создание датасетов и загрузчиков\n",
    "train_dataset = TextDataset(X_train_tokenized, y_train)\n",
    "test_dataset = TextDataset(X_test_tokenized, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TextDataset at 0x2433b60bf10>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2433b60b5e0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объяснение:\n",
    "* Создание датасета: Мы определяем класс TextDataset, который хранит токенизированные данные и метки.\n",
    "* DataLoader: Создаем загрузчики данных для обучающей и тестовой выборок. Батч размер установлен на 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Дообучение модели\n",
    "\n",
    "Fine-tuning выполняется с использованием оптимизатора и функции потерь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdamW — это улучшенная версия оптимизатора Adam, которая включает весовую регуляризацию (weight decay), что помогает предотвратить переобучение модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tqdm используется для создания прогресс-бара, который показывает ход выполнения цикла обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:40<00:00,  1.99it/s, loss=0.0105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 1: 0.03534754811080347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:40<00:00,  1.99it/s, loss=0.0028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 2: 0.03756050883785065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:40<00:00,  1.96it/s, loss=0.00272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 3: 0.11303430412372109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:40<00:00,  2.00it/s, loss=0.000862]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 4: 0.005740713713385049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:40<00:00,  1.98it/s, loss=0.000423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 5: 0.001011358002142515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine-tuned-ruBert\\\\tokenizer_config.json',\n",
       " './fine-tuned-ruBert\\\\special_tokens_map.json',\n",
       " './fine-tuned-ruBert\\\\vocab.txt',\n",
       " './fine-tuned-ruBert\\\\added_tokens.json',\n",
       " './fine-tuned-ruBert\\\\tokenizer.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Оптимизатор и параметры обучения\n",
    "# Создаем объект оптимизатора AdamW, который будет обновлять параметры модели (model.parameters()).\n",
    "# Параметр lr=5e-5 задает скорость обучения (learning rate). Это небольшое значение, типичное для fine-tuning моделей BERT.\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "epochs = 5 # Задаём количество эпох (проходов по всем данным) для обучения модели.\n",
    "\n",
    "# Цикл обучения\n",
    "for epoch in range(epochs):\n",
    "    model.train() # Переводим модель в режим обучения (train()).\n",
    "    total_loss = 0 # Инициализируем переменную total_loss для подсчета общей потери (loss) за текущую эпоху.\n",
    "    # Создаём прогресс-бар с помощью tqdm. Он будет отображать ход выполнения обучения для текущей эпохи.\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    # Начинаем цикл по батчам данных из train_loader. Каждый батч содержит токенизированные данные и метки классов.\n",
    "    for batch in progress_bar:\n",
    "        # Обнуляем градиенты перед каждым шагом обучения. Это необходимо, чтобы градиенты не накапливались между батчами.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Перемещение данных на устройство\n",
    "        # Перемещаем все элементы батча (например, input_ids, attention_mask, labels) на устройство (CPU или GPU).\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Прямой проход\n",
    "        # Выполняем прямой проход (forward pass) через модель. Передаем данные в модель с помощью оператора распаковки **batch.\n",
    "        # Модель возвращает объект outputs, который содержит логиты (logits) и значение функции потерь (loss).\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        # Извлекаем значение функции потерь (loss) из выходных данных модели. Это значение показывает, насколько сильно предсказания модели отличаются от правильных ответов.\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Обратное распространение ошибки\n",
    "        # Вычисляем градиенты функции потерь относительно параметров модели с помощью обратного распространения ошибки (backpropagation).\n",
    "        loss.backward()\n",
    "        # Обновляем параметры модели с использованием вычисленных градиентов. Это основной шаг оптимизации.\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Добавляем текущее значение потери (loss.item()) к общей потере за эпоху. loss.item() преобразует значение потери в число Python.\n",
    "        total_loss += loss.item()\n",
    "        # Обновляем прогресс-бар, отображая текущее значение потери (loss) для последнего батча.\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    # После завершения эпохи выводим среднее значение потери за все батчи.\n",
    "    # total_loss / len(train_loader) вычисляет среднюю потерю, разделяя общую потерю на количество батчей.\n",
    "    print(f\"Average loss for epoch {epoch + 1}: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Сохранение модели\n",
    "model.save_pretrained(\"./fine-tuned-ruBert\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-ruBert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объяснение:\n",
    "1. Оптимизатор: Используем AdamW с небольшой скоростью обучения (lr=5e-5).\n",
    "2. Цикл обучения: Модель обучается в течение нескольких эпох. На каждой итерации:\n",
    "* Выполняется прямой проход через модель.\n",
    "* Вычисляется ошибка.\n",
    "* Выполняется обратное распространение ошибки.\n",
    "3. Сохранение модели: После обучения модель сохраняется для дальнейшего использования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Оценка модели\n",
    "\n",
    "После обучения можно оценить качество модели на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9938\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Переводим модель в режим оценки (eval()).\n",
    "# В этом режиме отключаются такие механизмы, как dropout и batch normalization, которые используются только во время обучения.\n",
    "# Это важно для получения корректных предсказаний на тестовых данных.\n",
    "model.eval()\n",
    "\n",
    "# predictions: Будет содержать предсказанные классы модели.\n",
    "# true_labels: Будет содержать истинные метки классов из тестового набора данных.\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "# Используем контекстный менеджер torch.no_grad(), чтобы отключить вычисление градиентов.\n",
    "# Это уменьшает использование памяти и ускоряет процесс, так как градиенты не нужны при оценке модели.\n",
    "with torch.no_grad():\n",
    "    # Цикл по тестовым данным\n",
    "    # Начинаем цикл по батчам данных из тестового загрузчика (test_loader).\n",
    "    for batch in test_loader:\n",
    "        # Перемещаем все элементы батча (например, input_ids, attention_mask, labels) на устройство (CPU или GPU).\n",
    "        # Это необходимо, если модель работает на GPU, а данные находятся в памяти CPU.\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Выполняем прямой проход (forward pass) через модель. Передаем данные в модель с помощью оператора распаковки **batch.\n",
    "        # Модель возвращает объект outputs, который содержит логиты (logits).\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        # Извлекаем логиты из выходных данных модели. Логиты — это необработанные выходные значения модели перед применением softmax.\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Используем torch.argmax(logits, dim=-1) для выбора индекса с максимальным значением логита (т.е. предсказанного класса).\n",
    "        # .cpu().numpy() преобразует тензор PyTorch в массив NumPy, который можно использовать для дальнейших вычислений.\n",
    "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        \n",
    "        # Извлекаем истинные метки классов из батча. Они также преобразуются в массив NumPy.\n",
    "        labels = batch[\"labels\"].cpu().numpy()\n",
    "        \n",
    "        predictions.extend(preds) # Добавляем предсказанные классы в список predictions.\n",
    "        true_labels.extend(labels) # Добавляем истинные метки в список true_labels.\n",
    "\n",
    "# Используем функцию accuracy_score для вычисления точности модели:\n",
    "# Сравниваем предсказанные классы (predictions) с истинными метками (true_labels).\n",
    "# Результат — доля правильных предсказаний.\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "# Выводим точность модели с четырьмя знаками после запятой.\n",
    "# Например, если точность равна 0.8523, это означает, что модель правильно предсказала 85.23% примеров.\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объяснение:\n",
    "* Оценка: Модель используется для предсказания меток на тестовых данных.\n",
    "* Точность: Вычисляется точность (accuracy) с помощью accuracy_score из библиотеки sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код для тестирования предсказаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1: Как пройти в библиотеку в три часа ночи?\n",
      "Предсказанный класс: Консультации\n",
      "\n",
      "text2: При попытке открыть файл вылетает ошибка.\n",
      "Предсказанный класс: Ошибка/Проблема\n",
      "\n",
      "text3: Замените ноутбук. Он очень тупит.\n",
      "Предсказанный класс: Оборудование\n",
      "\n",
      "text4: Сделайте дополнительные рабочие места. Мало мест для работы.\n",
      "Предсказанный класс: Предложения по улучшению\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Загрузка сохранённой модели и токенизатора\n",
    "model_path = \"./fine-tuned-ruBert\"  # Укажите путь к сохранённой модели\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Маппинг меток обратно в текстовые значения\n",
    "label_map = {\n",
    "    0: \"Ошибка/Проблема\",\n",
    "    1: \"Консультации\",\n",
    "    2: \"Предложения по улучшению\",\n",
    "    3: \"Оборудование\"\n",
    "}\n",
    "\n",
    "# Функция для предсказания класса текста\n",
    "def predict(text):\n",
    "    # Токенизация входного текста\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,  # Ограничение длины последовательности\n",
    "        return_tensors=\"pt\"  # Возвращаем данные в формате PyTorch\n",
    "    )\n",
    "    \n",
    "    # Перемещение данных на устройство (CPU или GPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Предсказание\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "    \n",
    "    # Возвращаем текстовое значение класса\n",
    "    return label_map[predicted_class]\n",
    "\n",
    "# Пример тестирования\n",
    "test_texts = {\n",
    "    \"text1\": \"Как пройти в библиотеку в три часа ночи?\",\n",
    "    \"text2\": \"При попытке открыть файл вылетает ошибка.\",\n",
    "    \"text3\": \"Замените ноутбук. Он очень тупит.\",\n",
    "    \"text4\": \"Сделайте дополнительные рабочие места. Мало мест для работы.\"\n",
    "}\n",
    "\n",
    "# Вывод предсказаний\n",
    "for key, text in test_texts.items():\n",
    "    prediction = predict(text)\n",
    "    print(f\"{key}: {text}\\nПредсказанный класс: {prediction}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объяснение кода\n",
    "1. Загрузка модели и токенизатора\n",
    "* Мы загружаем предобученную модель и токенизатор из указанного пути (model_path).\n",
    "* Убедитесь, что путь к модели и токенизатору корректен.\n",
    "2. Маппинг меток\n",
    "* Создаем словарь label_map, который преобразует числовые метки (например, 0, 1, 2, 3) обратно в текстовые значения классов (например, \"Ошибка/Проблема\").\n",
    "3. Функция predict\n",
    "* Эта функция принимает текст, токенизирует его и передает в модель для предсказания.\n",
    "* Выполняется следующая последовательность:\n",
    "Токенизация: Преобразуем текст в токены с помощью токенизатора.\n",
    "Перемещение данных на устройство: Если доступна GPU, данные перемещаются на неё для ускорения вычислений.\n",
    "Предсказание: Модель вычисляет логиты (logits), а затем выбирается класс с наибольшим значением.\n",
    "Возврат текстового значения класса: Используем label_map для преобразования числового класса в текстовое значение.\n",
    "4. Тестирование\n",
    "* Мы создаем словарь test_texts, где ключи — это имена текстов, а значения — сами тексты.\n",
    "* Для каждого текста вызывается функция predict, и результат выводится на экран."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
