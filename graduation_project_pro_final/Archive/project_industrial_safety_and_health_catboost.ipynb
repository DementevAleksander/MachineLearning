{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2b039b3",
   "metadata": {},
   "source": [
    "# Прогнозирование уровня потенциального происшествия на производстве"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b765aae",
   "metadata": {},
   "source": [
    "<b>Цель.</b> <b>Оперативное принятие мер для повышения безопасности работников</b> на предприятиях, деятельность которых связана с вредными и опасными условиями труда, за счёт реализации модели машинного обучения, которая будет <b>предсказывать возможные уровни происшествий для конкретных работников</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db566d4",
   "metadata": {},
   "source": [
    "# Краткое описание проекта"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f9ff4f",
   "metadata": {},
   "source": [
    "<b>В ходе выполнения проекта:</b>\n",
    " * Провёл анализ датасета: проанализировал объём данных и его качество.\n",
    " * Выполнил EDA.\n",
    " * Выполнил очистку датасета (поиск и исключение имён работников) с помощью  <b>SpaCy</b>.\n",
    " * Применил <b>One-Hot Encoding</b> для категориальных признаков и <b>TF-IDF</b> для векторизации описания происшествия.\n",
    " * Обучил модель с помощью <b>Gradient Boosting</b> с подбором гиперпараметров <b>GridSearchCV</b>.\n",
    " * Обучил модель с помощью <b>MLP (Multilayer Perceptron)</b> с тремя слоями для обучения модели, функцией активации <b>ReLU</b> и <b>Dropout</b> для регуляризации и предотвращения переобучения.\n",
    " * Проанализировал и сравнил полученные результаты.\n",
    " * Поработал над датасетом, для улучшения результатов обучения (3 разных варианта датасета для Gradient Boosting и MLP).\n",
    " * Применил более продвинутые модели обучения <b>CatBoost</b> и <b>Word2Vec</b> (векторизация) для улучшения результатов обучения (метрик)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d705fafe",
   "metadata": {},
   "source": [
    "# Выполнение проекта"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f23cf1",
   "metadata": {},
   "source": [
    "Импортируем основные библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d31a372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter, defaultdict\n",
    "from collections.abc import Mapping\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f95eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Скрыть все предупреждения\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67ef117-7cf8-47a6-8059-5c3f82289858",
   "metadata": {},
   "source": [
    "<b>Приведём описание признаков:</b>\n",
    "* <b>Data:</b> временная метка или информация о времени/дате\n",
    "* <b>Countries:</b> в какой стране произошла авария (анонимно)\n",
    "* <b>Local:</b> город, в котором находится завод-изготовитель (анонимно)\n",
    "* <b>Industry sector:</b> к какому сектору относится завод\n",
    "* <b>Accident level:</b> от I до VI показывает, насколько серьезным было происшествие (I означает \"лёгкое\", а VI - \"очень тяжёлое\")\n",
    "* <b>Potential Accident Level:</b> насколько серьезным могло бы быть происшествие (из-за других факторов, связанных с происшествием)\n",
    "* <b>Genre:</b> пол (мужчина/женщина)\n",
    "* <b>Employee or Third Party:</b> является сотрудник штатным или третьей стороной (подрядчик)\n",
    "* <b>Critical Risk:</b> краткое описание риска, связанного с происшествием\n",
    "* <b>Description:</b> подробное описание того, как произошло происшествие."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823c3157",
   "metadata": {},
   "source": [
    "Считываем данные из CSV-файла. Выводим первые 5 строк, чтобы убедиться, что данные считались с локального файла."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db85b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('database_with_accidents_description.csv')\n",
    "pd.set_option('display.max_colwidth', None) # Настраиваем отображение, чтобы текст выводился полностью\n",
    "pd.set_option('display.max_columns', None) # Настраиваем отображение, чтобы отображалдись все колонки\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a91c523",
   "metadata": {},
   "source": [
    "Посмотрим на количество записей в файле."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc71864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c1e7ff",
   "metadata": {},
   "source": [
    "Посмотрим, какого типа эти данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a2f2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b86d96",
   "metadata": {},
   "source": [
    "Видим, что пропущенные значения отсутствуют. Есть категориальные признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a37e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Просмотр количества уникальных записей для каждого признака\n",
    "df.apply(lambda x: x.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79ea274",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1518a0",
   "metadata": {},
   "source": [
    "Выберем целевую переменную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dae51c-64fb-4644-8fb2-220ad3803eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Данные для первого графика\n",
    "accident_level_counts = df['Accident Level'].value_counts().sort_index()\n",
    "# Данные для второго графика\n",
    "potential_accident_level_counts = df['Potential Accident Level'].value_counts().sort_index()\n",
    "\n",
    "# Первый график (Зелёный прозрачный)\n",
    "ax.bar(accident_level_counts.index, accident_level_counts, color='blue', alpha=0.5, label='Accident Level')\n",
    "\n",
    "# Второй график (Красный прозрачный)\n",
    "ax.bar(potential_accident_level_counts.index, potential_accident_level_counts, color='red', alpha=0.5, label='Potential Accident Level')\n",
    "\n",
    "plt.title('Соотношение значений целевой переменной')\n",
    "plt.xlabel('Значение целевой переменной')\n",
    "plt.ylabel('Количество')\n",
    "\n",
    "# Подписи для первого графика\n",
    "for i, value in enumerate(accident_level_counts):\n",
    "    plt.text(i, value, str(value), ha='center', va='bottom', color='blue')\n",
    "\n",
    "# Подписи для второго графика\n",
    "for i, value in enumerate(potential_accident_level_counts):\n",
    "    plt.text(i, value, str(value), ha='center', va='bottom', color='red')\n",
    "\n",
    "# Отображение легенды\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d64f466",
   "metadata": {},
   "source": [
    "<b>В данном случае будем использовать потенциальный уровень происшествия (Potential Accident Level), как целевую переменную (метку), т.к. для бизнеса прогнозировать это значение более важно, чем фактический уровень происшествия.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fa0941-ed84-4304-ae3d-38c735505906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Группировка данных и подсчет количества записей по признаку \"Critical Risk\" и уровню \"Potential Accident Level\"\n",
    "grouped_data = df.groupby(['Critical Risk', 'Potential Accident Level']).size().unstack(fill_value=0)\n",
    "\n",
    "# Сортировка по убыванию суммы значений в строках\n",
    "grouped_data = grouped_data.loc[grouped_data.sum(axis=1).sort_values(ascending=False).index]\n",
    "\n",
    "# Построение графика\n",
    "grouped_data.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "\n",
    "# Настройка заголовков и меток осей\n",
    "plt.title('Количество записей \"Potential Accident Level\" по признаку \"Critical Risk\"')\n",
    "plt.xlabel('Critical Risk')\n",
    "plt.ylabel('Количество записей')\n",
    "plt.legend(title='Potential Accident Level')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Отображение графика\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9173822-e169-4979-941b-8efd7151c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фильтрация данных, исключая значение 'Others' в столбце 'Critical Risk'\n",
    "filtered_df = df[df['Critical Risk'] != 'Others']\n",
    "\n",
    "# Группировка данных и подсчет количества записей по признаку \"Critical Risk\" и уровню \"Potential Accident Level\"\n",
    "grouped_data = filtered_df.groupby(['Critical Risk', 'Potential Accident Level']).size().unstack(fill_value=0)\n",
    "\n",
    "# Сортировка по убыванию суммы значений в строках\n",
    "grouped_data = grouped_data.loc[grouped_data.sum(axis=1).sort_values(ascending=False).index]\n",
    "\n",
    "# Построение графика\n",
    "grouped_data.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "\n",
    "# Настройка заголовков и меток осей\n",
    "plt.title('Количество записей \"Potential Accident Level\" по признаку \"Critical Risk\"')\n",
    "plt.xlabel('Critical Risk')\n",
    "plt.ylabel('Количество записей')\n",
    "plt.legend(title='Potential Accident Level')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Отображение графика\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877feda1-3323-48fa-99b1-51a20500aa0b",
   "metadata": {},
   "source": [
    "Посмотрим распределение происшествий по месяцам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e796206-fb7e-4a4b-a702-84668c524975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование столбца Data в тип datetime\n",
    "df['Data'] = pd.to_datetime(df['Data'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Извлечение названия месяца\n",
    "df['Month'] = df['Data'].dt.strftime('%B')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefa1b5c",
   "metadata": {},
   "source": [
    "Удаляем ненужные колонки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c17649b-2620-480b-8f7b-cb4fe49fecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"Unnamed: 0\", \"Data\", \"Countries\", \"Local\", \"Industry Sector\", \"Accident Level\",]\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75b4054-e90c-4853-a7dd-684d9cb95d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978c5d09-d180-4436-9559-bd354fef43f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем словарь с порядком сортировки месяцев\n",
    "month_order = {\n",
    "    'January': 1, 'February': 2, 'March': 3, 'April': 4,\n",
    "    'May': 5, 'June': 6, 'July': 7, 'August': 8,\n",
    "    'September': 9, 'October': 10, 'November': 11, 'December': 12\n",
    "}\n",
    "\n",
    "# Выполняем подсчет и сортировку\n",
    "month_counts = df['Month'].value_counts().sort_index(\n",
    "    key=lambda x: x.map(month_order)\n",
    ")\n",
    "\n",
    "# Создание графика\n",
    "plt.figure(figsize=(10, 6))  # Размер графика\n",
    "\n",
    "# Построение столбчатой диаграммы\n",
    "month_counts.plot(kind='bar', color='skyblue')\n",
    "\n",
    "# Настройка заголовка и меток осей\n",
    "plt.title('Количество записей по месяцам')\n",
    "plt.xlabel('Месяц')\n",
    "plt.ylabel('Количество записей')\n",
    "\n",
    "# Отображение графика\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f9db7-2504-4e30-b016-f717510f35e1",
   "metadata": {},
   "source": [
    "# Нормализуем и почистим текстовое описание происшествий \"Description\" от имён работников."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790013de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80aecfe-d816-4eea-93d9-d0ec609df01a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Загружаем модель spaCy для NER и лемматизации\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Функция для очистки текста\n",
    "def clean_description(text):\n",
    "    doc = nlp(text)\n",
    "    names = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "    clean_text = text\n",
    "    for name in names:\n",
    "        clean_text = re.sub(r'\\b' + re.escape(name) + r'\\b', '', clean_text)\n",
    "    \n",
    "    # Преобразование текста в нижний регистр\n",
    "    clean_text = clean_text.lower()\n",
    "    \n",
    "    # Удаление стоп-слов\n",
    "    clean_text = ' '.join([word for word in clean_text.split() if word not in STOP_WORDS])\n",
    "    \n",
    "    # Удаление коротких слов\n",
    "    clean_text = ' '.join([word for word in clean_text.split() if len(word) > 2])\n",
    "    \n",
    "    # Удаление чисел\n",
    "    clean_text = re.sub(r'\\b\\d+\\b', '', clean_text)\n",
    "    \n",
    "    # Удаление пунктуации\n",
    "    clean_text = clean_text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    return clean_text, names\n",
    "\n",
    "# Функция для лемматизации текста\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_text = ' '.join([token.lemma_ if token.lemma_ != '-PRON-' else token.text for token in doc])\n",
    "    return lemmatized_text\n",
    "\n",
    "# Функция для удаления лишних пробелов\n",
    "def remove_extra_spaces(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "# Применяем функцию ко всему столбцу Description и сохраняем удаленные имена\n",
    "df['Cleaned_Description'], df['Removed_Names'] = zip(*df['Description'].apply(clean_description))\n",
    "\n",
    "# Применяем лемматизацию к столбцу с очищенным описанием\n",
    "df['Cleaned_Description'] = df['Cleaned_Description'].apply(lemmatize_text)\n",
    "\n",
    "# Удаляем лишние пробелы\n",
    "df['Cleaned_Description'] = df['Cleaned_Description'].apply(remove_extra_spaces)\n",
    "\n",
    "# Выводим имена, которые были удалены\n",
    "for idx, names in enumerate(df['Removed_Names']):\n",
    "    if names:\n",
    "        print(f\"Row {idx}: Removed names - {names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b07f597-fb92-4c5a-b331-ac83fe01e6b2",
   "metadata": {},
   "source": [
    "# Перекодируем уровень происшествия в числовые значения и сохраним в новый датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6139d7d2-99ef-408b-b07e-2182ea615d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем объект LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Применяем LabelEncoder к столбцу 'Potential Accident Level'\n",
    "df['Potential Accident Level Encoded'] = label_encoder.fit_transform(df['Potential Accident Level']) + 1\n",
    "\n",
    "# Маппинг значений после кодирования\n",
    "level_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_) + 1))\n",
    "print(\"Mapping for Potential Accident Level:\")\n",
    "print(level_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3c61ed-fc5a-4a52-baf0-c0ecec9768db",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"Removed_Names\", \"Description\", \"Potential Accident Level\",]\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e918b-76cf-4e74-8a73-edca400dfa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем очищенный и нормализованный датасет\n",
    "df.to_csv('descr_cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d3fb4f",
   "metadata": {},
   "source": [
    "# Предобработка датасета для обучения. Вариант 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488b73bc-e17a-422a-90ac-41dd7acb50ba",
   "metadata": {},
   "source": [
    "# Выполним One-Hot Encoding для категориальных признаков и векторизацию описания происшествия с использованием TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f5eb12-190d-492b-8a49-9effeb8c2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('descr_cleaned_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479a134-aa91-4ad2-8b99-d709a7d41eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "df_copy['Cleaned_Description'] = df_copy['Cleaned_Description'].apply(lambda x: (x[:197] + '...') if len(x) > 200 else x)\n",
    "df_copy.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1358fc6",
   "metadata": {},
   "source": [
    "Применяем One-Hot Encoding с помощью pd.get_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba20b1bd-8488-4ca6-bf93-87339a90d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['Genre', 'Employee or Third Party', 'Critical Risk', 'Month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e97ef98-b226-41db-ad63-64be608e399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "df_copy['Cleaned_Description'] = df_copy['Cleaned_Description'].apply(lambda x: (x[:47] + '...') if len(x) > 50 else x)\n",
    "df_copy.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d961951",
   "metadata": {},
   "source": [
    "Выполняем векторизацию текста с использованием TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf99b767-a8f9-41bb-9c35-5151e76faf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Векторизация текста с использованием TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=10000)\n",
    "X_tfidf = tfidf.fit_transform(df['Cleaned_Description'])\n",
    "\n",
    "# Преобразование в DataFrame для объединения с исходным датасетом\n",
    "X_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d525c4-6763-498d-8195-1861e6560160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединение признаков\n",
    "df = pd.concat([df.drop(columns=['Cleaned_Description']), X_tfidf_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b0f55-1aa6-4683-a5e8-5a2576b1f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c23abb5-4691-4d0f-b7c3-43b892150257",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a788e39-81aa-4096-bd3c-b4e7223f2b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удалим происшествия 6й категрории, т.к. их всего 1 штука.\n",
    "df = df[df['Potential Accident Level Encoded'] != 6]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca02d27-f5bc-4f45-ae03-6f4a69f3a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделяем признаки и метки\n",
    "X = df.drop(columns=['Potential Accident Level Encoded'])\n",
    "y = df['Potential Accident Level Encoded']\n",
    "\n",
    "# Разбиение на тренировочную и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae233f26",
   "metadata": {},
   "source": [
    "# Обучение. Градиентный бустинг. Вариант 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017bd1bd-474f-44d9-b2e0-25e1684b7045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение гиперпараметров для подбора\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# Настройка GridSearchCV\n",
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=gb_clf, param_grid=param_grid, cv=3, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "# Обучение модели с подбором гиперпараметров\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Лучшие параметры\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Оценка модели с лучшими параметрами\n",
    "best_gb_clf = grid_search.best_estimator_\n",
    "y_pred_gb = best_gb_clf.predict(X_test)\n",
    "\n",
    "# Получение меток классов из y_test\n",
    "class_labels = sorted(set(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646574d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Оценка модели Gradient Boosting\n",
    "gb_report = classification_report(y_test, y_pred_gb, target_names=[f\"{i+1}\" for i in class_labels])\n",
    "print(\"Gradient Boosting Classification Report:\")\n",
    "print(gb_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6460e21e",
   "metadata": {},
   "source": [
    "# Обучение. Нейронная сеть. Вариант 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcca80c-4a22-409b-9473-026a65922519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование всех столбцов в числовой формат\n",
    "def convert_to_numeric(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'bool':\n",
    "            df[col] = df[col].astype(int)\n",
    "        elif df[col].dtype == 'object':\n",
    "            try:\n",
    "                df[col] = df[col].astype(float)\n",
    "            except ValueError:\n",
    "                df[col] = pd.factorize(df[col])[0]\n",
    "    return df\n",
    "\n",
    "# Выделяем признаки и метки\n",
    "X = df.drop(columns=['Potential Accident Level Encoded'])\n",
    "y = df['Potential Accident Level Encoded']\n",
    "\n",
    "# Преобразование всех данных в числовой формат\n",
    "X = convert_to_numeric(X)\n",
    "\n",
    "# Разбиение на тренировочную и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Преобразование меток классов в диапазон от 0 до num_classes-1\n",
    "y_train = y_train - 1\n",
    "y_test = y_test - 1\n",
    "\n",
    "# Преобразование данных в тензоры\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Создание DataLoader для PyTorch\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Определение модели нейронной сети\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)  # уменьшение размера скрытого слоя\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, num_classes)\n",
    "        self.dropout = nn.Dropout(0.6)  # увеличение dropout\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.batch_norm2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 128  # уменьшение скрытого слоя\n",
    "num_classes = len(y.unique())\n",
    "\n",
    "model = MLP(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Определение функции потерь и оптимизатора с L2-регуляризацией\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.01)  # уменьшение learning rate и увеличение weight decay\n",
    "\n",
    "# Обучение модели с ранним прекращением\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "best_test_loss = float('inf')\n",
    "patience = 10  # Число эпох без улучшений перед остановкой\n",
    "early_stopping_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "    # Проверка на тестовом наборе\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    test_losses.append(test_loss / len(test_loader))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}\")\n",
    "\n",
    "    # Раннее прекращение\n",
    "    if test_losses[-1] < best_test_loss:\n",
    "        best_test_loss = test_losses[-1]\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# Оценка модели\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.extend((predicted + 1).numpy())  # Сдвиг предсказанных значений на +1\n",
    "        y_true.extend((y_batch + 1).numpy())    # Сдвиг истинных значений на +1 для правильного сравнения\n",
    "\n",
    "# Проверка на переобучение и недообучение\n",
    "plt.plot(range(len(train_losses)), train_losses, label='Train Loss')\n",
    "plt.plot(range(len(test_losses)), test_losses, label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Train and Test Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb259d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка модели Gradient Boosting\n",
    "# gb_report = classification_report(y_test, y_pred_gb)\n",
    "print(\"Gradient Boosting Classification Report:\")\n",
    "print(gb_report)\n",
    "\n",
    "# Отчет о классификации\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(\"Neural Network Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd624d8-0b26-4886-a894-09d66004e75a",
   "metadata": {},
   "source": [
    "# Предобработка датасета для обучения. Вариант 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27f3a8",
   "metadata": {},
   "source": [
    "<h3>Для каждого класса собирём все описания происшествий в единый текст, затем определим из этого текста:</h3> \n",
    "<br>1. Должность.\n",
    "<br>2. Действия, которые совершал работник.\n",
    "<br>3. Инструменты и техника.\n",
    "\n",
    "Добавим эти данные для каждой строки соответствующего класса в датасете (создадим новые поля)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2e108a-55e5-4523-947b-dba45d55eb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('descr_cleaned_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e82254d-9ec6-4873-b152-d5ddfbbe54b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Группировка описаний по категориям\n",
    "grouped = df.groupby('Potential Accident Level Encoded')['Cleaned_Description'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "grouped.to_excel('c:/Users/AADementev/Desktop/Projects/python/MachineLearning/graduation_project_pro_final/grouped_descriptions.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453bfe3f-fc60-44bf-8fa2-61a9928cc214",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbd1e2d-e920-4e08-a8f4-107451c8bf63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_descriptions = dict(zip(grouped['Potential Accident Level Encoded'], grouped['Cleaned_Description']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7268a0a-e4cd-4592-a716-ba13ee577584",
   "metadata": {},
   "source": [
    "<h3>Попробуем сделать автоматическую выборку должностей, действий и минструментов с помощью библиотеки spaCy для обработки естественного языка (NLP).</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e08e16-e4d7-4487-b9d6-bcc55b3fdedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем модель для обработки текста на английском языке\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Текст\n",
    "text = \"\"\"\n",
    "be approximately nv cx ob7 personnel begin task unlock soquet bolt bhb machine penultimate bolt identify hexagonal head wear proceed mr auxiliary assistant climb platform exert pressure hand dado key prevent come bolt moment collaborator rotate lever anticlockwise direction leave key bolt hit palm leave hand cause injury collaborator report work ustulación realize cyclone duct obstruct open door try unclog material detach project employee cause small burn right heel end lunch enable place winche control room get short walk slip sit floor make contact leave knee take importance rest guard guard finish go safety communicate fact reason derive natclar attention trip vehicle end work collaborator step object identify come pierce sole boot cause small hole sole left foot collaborator perforation possibly stump wood area cover collaborator pasture graze recently near residence perform geological mapping activity necessary hammer rock analysis moment clerk hold it point fragment slip quirodactyl right hand cause superficial cut circumstance collaborator perform washing tabolas pot washing area suffer feel dizziness faintness cause fall level produce slight concussion head ground team coordinate prospector assistant wila pm prong opening access collect soil sample come try divert meter right place moment diversion come marimbondo house he give time action thug agitate time ste head neck sting face allergy test verify allergic reaction wash affect return normal activity geological reconnaissance activity farm mr team compose felipe normal activity encounter ciliary forest need enter forest verify rock outcrop front divine realize open access machete moment take bite neck attack allergic reaction continue work normally work complete leave forest access divine assistant attack snake suffer sting forehead moment move away area verify type allergic reaction return normal activity geological reconnaissance activity farm mr team compose felipe normal activity encounter ciliary forest need enter forest verify rock outcrop front divine realize open access machete moment take bite neck attack allergic reaction continue work normally work complete leave forest access divine assistant attack snake suffer sting forehead moment move away area verify type allergic reaction return normal activity employee work electrician management electrometallurgy suffer contusion right leg suffer slip height step staircase code ele abb furnace cat ladder immediately refer collaborator medical service treat circumstance employee connection electric cable no jumbo operator feel discomfort face clean hand rubber glove generate superficial laceration small wound leave cheekbone project vazante carry sediment collection current south mata target drainage serra garrote team compose member wca company move collection point another inside shallow drainage see bee carton reaction away box quickly possible avoid sting run meter look safe area exit radius attack bee ss breno attack consequently suffer sting belly hand verify type allergic reaction return normal activity project vazante carry sediment collection current south mata target drainage serra garrote team compose member wca company move collection point another inside shallow drainage see bee carton reaction away box quickly possible avoid sting run meter look safe area exit radius attack bee ss breno attack consequently suffer sting belly hand verify type allergic reaction return normal activity geologo auxiliary travel evaluate geological point follow gps near drainage follow state highway give access area stop get vehicle point identify gps distance seven meter vehicle follow road surprised bite thorn face neck quickly hurried vehicle move away place clerk wear girdle goggle wear glove enter forest area allergic reaction geologist auxiliary travel field evaluate geological point follow gps near drainage follow state highway give access area stop get vehicle point identify gps mário distancing meter vehicle follow road surprised bite thorn face quickly hurried vehicle move away place clerk wear girdle goggle wear glove enter forest area allergic reaction safety technical move field inspection activity way field pause team order know drainage point check safety get vehicle strike sting weed neck quickly return vehicle radio communication team distance place clerk wear legging glass allergic reaction auxiliary travel field evaluate geological point follow gps near drainage follow state highway give access area stop get vehicle point identify gps distancing meter vehicle accompany geologist surprised bite blow neck quickly hurried vehicle move away place clerk wear girdle goggle wear glove enter forest area allergic reaction travel field order geological mapping geologist accompany prospector stoop deviate vegetation time receive whistling sting they face neck allergic reaction activity follow normally event move field geological mapping prospector accompany geologist stoop deviate vegetation moment receive whistle sting ring finger right hand allergic reaction activity follow normally event mince team carry activity city juína coordinate mining technician felipe time mining technician line away team bite blackjack leave face allergic manifestation team continue work afternoon lunch employee seek medical care medicate release continue activity day level unicon plant collaborator shuttering work concrete water sedimentation basin moment nail wood supply inch strip feel metallic hammer loosen wooden handle fix it grab hammer head hit handle vertically wood generating injury time accident employee use safety glove cut vegetation open bite sickle assistant strike vine twice liana ruptured branch project face auxiliary cause cut upper lip collaborator be clean leave return borehole brapdd slip canva edge well hit right metal structure mudswathe box cause slight excoriation employee refer local hospital medicate release activity be approximately mechanic remove bolt nipple pump lime feeder reactive area mechanic position slightly flex leg perform upward force hand moment feel pain spume right thigh mechanic evacuate help colleague medical post region povoado vista martinópole ce employee perform soil collection activity field auxiliary diassis nascimento be cross fence glove attach wire body project forward cause slight twist leave wrist team travel city granja employee refer hospital consultation doctor diagnose fracture prescribing remedy local pain ice pack medical evaluation employee carry activity normally level dining room collaborator finish wash tabolas food container dimension cm proceed order pink thumb right hand corner aluminum tabola generating lesion employee time accident safety glove preparation scaffold activity employee loading piece designate place finger press metal piece move employee engage removal material excavation level shovel placing bucket day material fall pipe employee boot friction boot calf cause superficial injury leg employee engage removal material excavation level shovel placing bucket day material fall pipe employee boot friction boot calf cause superficial injury leg be activity collect soil collaborator run branch attack maribondo bite twice head pain swell allergic symptom continue activity activity package cylindrical piece easel employee carry piece designate place finger press metal piece move perform carpentry work collaborator hit second finger leave hand hammer hold right hand cause bruise height nail evaluation carry medical center unit final diagnosis contusion finger pm perform magnetometric gps collaborator bump field hat branch attack maribondo bitten ear shoulder continued activity feel pain swelling pm assist gps magnetometric collaborator bump field hat branch attack maribondo moth go eye use sunglass attack region prevent insect move face getting catch ear field hat make helper bite ear allergic marimbondo bite soon activity immediately paralyze drove car accident take medicine antiallergic situation work indicate doctor avoid swell responsible project field mapping activity call radio immediately assistant feel good take emergency hospital lavra sul consult doctor take antiallergic release carry refractory brick chop activity order place support bus bar section particle detach hit assistant right arm meter away work area provoke wound arm treat medical center return usual duty soil sample region employee danillo silva attack bee test rush away place employee take bite chin chest neck hand glove employee take bite hand glove head employee danillo take bite leave arm uniform sketch allergy swell ste site activity stop evaluate site verifying test remain line leave site soil sample region employee danillo silva attack bee test rush away place employee take bite chin chest neck hand glove employee take bite hand glove head employee danillo take bite leave arm uniform sketch allergy swell ste site activity stop evaluate site verifying test remain line leave site soil sample region employee danillo silva attack bee test rush away place employee take bite chin chest neck hand glove employee take bite hand glove head employee danillo take bite leave arm uniform sketch allergy swell ste site activity stop evaluate site verifying test remain line leave site pm perform mag activity employee move acquisition line come small drainage approximately 40 cm wide small gap traverse drainage employee rest right foot ravine come rest cause right ankle twist soon twist activity paralyze employee take local hospital xray take examination physician injury find small swelling release normal activity team vms project perform soil collection xixás target members team move collection point another mr ahead team sting near collection point surprised swarm bee inside play near ground visibility wood hiss noise pass stump attack bee ste left arm uniform prick lip screen rip tangle branch escape team vms project perform soil collection xixás target members team move collection point another mr ahead team sting near collection point surprised swarm bee inside play near ground visibility wood hiss noise pass stump attack bee ste left arm uniform prick lip screen rip tangle branch escape technician magnetometric survey step thorn reaction immediately retreat lose balance magnetometer antenna break 30hs current sediment activity collaborator take bee ste neck screen bee enter screen sting team decide leave workplace presence bee collaborator reaction continue work normally execution soil sample task potion area pm open machete bite wasp right hand time incident epi needed activity employee evaluate technician find mild localize swelling wound employee report feel pain continue activity execution service opening prick future work ip employee line equipment sting wasp right portion neck beetle small size see employee bite cause employee shock insect manifest employee ppe require activity develop bite occur collar shirt face shield technician responsible performing work evaluate ste and injure employee find localized swell allergy need paralyze activity follow normally execution soil sample task potion area pablo move bite bite right elbow wasp sleeve uniform time incident ppe need activity employee evaluate team find mild injury localize swell employee report feel pain continue activity employee pass corner door see virdro slight swell frontal region closing glass door activity maintenance scaller breaker arm extension cylinder local underground level removal cylinder scaller arm releasing fix pin cylinder come down bump tool press hand tool structure equipment field activity amg project target reconnaissance team boarding car park window close enter mr put seat belt inside vehicle press wasp shoulder neck cause sting believe that possibly bee nail clothe car properly close\n",
    "\"\"\"\n",
    "\n",
    "# Функция для извлечения должностей\n",
    "def extract_positions(text):\n",
    "    positions = set()\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"PERSON\", \"ORG\"]:\n",
    "            positions.add(ent.text)\n",
    "    return list(positions)\n",
    "\n",
    "# Функция для извлечения оборудования и инструментов\n",
    "def extract_equipment(text):\n",
    "    equipment = set()\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            equipment.add(token.text)\n",
    "    return list(equipment)\n",
    "\n",
    "# Функция для извлечения действий\n",
    "def extract_actions(text):\n",
    "    actions = set()\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            actions.add(token.lemma_)\n",
    "    return list(actions)\n",
    "\n",
    "# Извлечение данных\n",
    "positions = extract_positions(text)\n",
    "equipment = extract_equipment(text)\n",
    "actions = extract_actions(text)\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"Должности:\")\n",
    "print(positions)\n",
    "print(\"\\nДействия:\")\n",
    "print(actions)\n",
    "print(\"\\nОборудование и инструменты:\")\n",
    "print(equipment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27408ce4",
   "metadata": {},
   "source": [
    "Как видим результаты не очень точные."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3075ff0f-a118-4c71-a4bb-f2740cdd8a00",
   "metadata": {},
   "source": [
    "<h3>Выполним сбор данных вручную.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a776e8d-fa7f-45ff-9e25-0ec2064de00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('descr_cleaned_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a9e16d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32240316-5169-4b71-b54e-974cfb5f0604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем словарь с данными для таблицы\n",
    "data = {\n",
    "    \"Должности\": [\n",
    "        \"Auxiliary assistant, Collaborator, Clerk, Geologist, Safety technical, Mining technician, Mechanical, Danilo Silva, Technician.\", \n",
    "        \"Forklift operator, Collaborator, Operator, Chief guard, Assistant, Engineer trainee, Teacher, Store attendant, Welder, Topographic surveyor, Operator bolter, Technician, Mixkret operator, Geologist, Maintenance team, Mechanic, Surveying worker, Industrial cleaning worker, Food preparer, Mobile equipment maintenance team, Comedor worker.\", \n",
    "        \"Collaborator, Mechanic technician, Electrician supervisor, Master loader, Equipment assistant, Official operator, Assistant loader, Master shipper, Truck crane operator, Mechanic, Technician, Civil operator, Bolted assistant, Welder, Helper, Boltec technician, Sampler, Electrician, Operator, Mechanic operator.\", \n",
    "        \"Maintenance Supervisor, Mechanic, Driller assistant, Operator, Mason Assistant, Bolter Operator, Mill Operator, Assistant Mechanic, Welder, Technician, Truck Driver, Plant Worker, Blaster, Maintenance worker.\", \n",
    "        \"Mixkret operator, Resident engineer, Filter operator, Autoclave operator, Mechanic, Pilot, Mixer operator, Wheel loader operator, Operator of the concrete plant, Scalar operator.\"\n",
    "    ],\n",
    "    \"Должности (пример на русском)\": [\n",
    "        \"Геолог, Горный техник-технолог.\", \n",
    "        \"Оператор вилочного погрузчика, Оператор горно-шахтной самоходной машины.\", \n",
    "        \"Техник-механик, Оператор автокрана.\",\n",
    "        \"Помощник по бурению, Взрыватель.\", \n",
    "        \"Пилот, Оператор фронтального погрузчика, Оператор бетономешалки, Оператор горно-шахтной самоходной машины, Механик.\"\n",
    "    ],\n",
    "    \"Действия\": [\n",
    "        \"Unlock, Identify hexagonal head, Climb platform, Exert pressure hand, Try unclog material, Detach project, Enable place winch control room, Get short walk slip, Sit floor, Make contact, Carry sediment collection, Run meter, Evaluate geological point, Give access area, Know drainage point, Strike sting weed neck, Perform soil collection activity, Attach wire, Perform magnetometric GPS, Use sunglass, Carry refractory brick, Chop activity, Place support bus bar section, Execute soil sample task, Open machete, Break magnetometer Antenna, Move acquisition line, Traverse drainage.\", \n",
    "        \"Cleaning metal structures, Operating a crane and forklift, Using a hammer and chisel, Releasing a blade and manually displacing sheets, Operating a truck and adjusting bolts, Cutting electro welded mesh, Opening access areas with tools, Welding and inspecting mining cars, Handling chemicals, Performing maintenance on various equipment, Using a torch for cutting activities, Loading and unloading materials, Preparing geological maps and conducting surveys, Painting floors, Carrying out inspections, Preparing food, Cleaning industrial areas, Handling ventilation equipment.\", \n",
    "        \"Excavation work, Unload operation, Unclog discharge mouth, Maneuver to unhook hose, Turn pulley manually, Grab transmission belt, Verify belt tension, Install segment of polyurethane pulley, Clean shutter with air lance, Perform truck unload operation, Remove rope tie, Perform disconnection of power cable, Detach upper support point, Verify remaining position, Identify rock mesh, Change drill bit, Release coupling, Replace telescopic expansion joint, Position portable ladder, Hold base of loader, Clean spatula spear window boiler, Perform radial drilling, Activate hydraulic pump inspection cover, Install support mesh cloth, Handle water supply hose, Verify lock failure, Dismantle scaffold, Perform carbon steel pipe mark activity, Perform maintenance on motor support, Place protective plate on fuel tank, Perform supply operation for zinc powder container, Perform maintenance activity on transmission belt, Test soft starter engine belt, Perform mechanical support activity, Conduct inspection of sulfuric acid spill line, Carry out sand electrolysis piece, Perform brushcutter operation, Handle pneumatic conveyor, Transport dust zinc container, Lower metal sheet, Assemble activity for polypropylene pipe, Clean area near conveyor, Move locomotive personnel, Perform drilling activity with LM17 probe, Remove bucket of pulp sample, Supervise ustulation activity, Carry inspection cut block level OB6A.\", \n",
    "        \"Loosening support, Facilitating removal, Tightening, Activating pump, Designing area, Applying aid, Positioning pot, Operating drill, Cleaning position, Securing pipe, Lifting platform, Aligning cathode press, Manually moving steel cabinet, Drilling hole, Removing flange, Applying shotcrete, Preparing oil cylinder, Mounting rail platform, Feeding bag into furnace, Supporting stabilizer, Conducting maintenance, Checking work front, Verifying ventilation, Placing mesh, Pulling support mesh, Unloading ore, Reshaping hand, Unlocking rod, Tightening bolt, Evaluating acid leakage, Unloading residual water, Operating equipment, Driving truck, Inspecting equipment, Removing suction pipe, Cleaning low floor, Wearing safety equipment, Entering filter belt, Preparing construction, Perform rock untie, Lift brace, Hoisting and setting up equipment, Changing fuses, Soil collection, Cleaning area, Installation of ventilation plug, Welding steel plate, Unloading material, Operating overflow system, Manipulating hose, Testing equipment.\", \n",
    "        \"Open the electric board, Proceed with the installation, Remove the lock, Use the thermomagnetic key, Make phase contact, Check voltage, Plug socket, Cut wire, Transfer pump, Clean pump, Manipulate motor pump transmission, Change cable, Start equipment, Stop equipment, Perform sanitation, Clean compressor, Lubricate equipment, Load explosive equipment, Putty work, Clean material, Change conveyor belt.\"\n",
    "    ],\n",
    "    \"Действия (пример на русском)\": [\n",
    "        \"Крепление провода, Установка опорной шины, Отбор пробы почвы, Включение пункта управления лебёдкой, Прокладка дренажа.\", \n",
    "        \"Очистка металлических конструкций, Управление краном и вилочным погрузчиком, Сварка и осмотр карьерных машин, Погрузка и разгрузка материалов, Использование горелки для резки материалов.\", \n",
    "        \"Разгрузка грузовика, Радиальное бурение, Отсоединение кабеля питания, Разъединение муфты, Замена телескопического компенсатора.\", \n",
    "        \"Установка резервуара, Монтаж рельсовой платформы, Подготовка масляного баллона, Подача мешка в печь, Установка стабилизатора.\", \n",
    "        \"Проверка напряжения, Очистка насоса, Замена кабеля, Очистка компрессора, Загрузка взрывоопасного оборудования, Замена ленты конвейера.\"\n",
    "    ],\n",
    "    \"Инструменты и техника\": [\n",
    "        \"Metal piece, Shovel, Bucket, Canvas, Wooden handle, Metallic hammer, Sickle, Electric cable, Magnetometric GPS, Antenna, Hammer, Rock, Machete, Refractor, Shovel, Tabola, Scaller breaker arm, Hand tool, Structure equipment, Magnetometer, Screen.\", \n",
    "        \"Forklift, Hammer and chisel, Ladder, Manual displacement tools, Truck (A30), Electro welded mesh cutter, Machete, Welding equipment, Mixing equipment, Loaders (e.g., J005A), Sludge lever, Shears, Scissor bolter, Mona car, Laboratory sampling tools, Chemical containers, Pressure hoses, Pumps and blowers, Mining and inspection tools, High-pressure pump gun, Shovels and cleaning tools, Power cables and sockets, Cathode cranes, Zinc sheets, Wooden stumps, Rail tracks and corridors, Industrial cleaning equipment, Soil activity tools (e.g., pickaxe).\", \n",
    "        \"Ustilago powder, Silo truck, Transmission belt HM pump, Polyurethane pulley, Electro welded mesh, Hydraulic cylinder, Winch pulley, Air lance, Telescopic ladder, Metal bar hammer, Telescopic expansion joint HDPE pipe, Spatula spear window boiler, Simba M4C ITH equipment, Hydraulic pump, Volumetric balloon, Radial drilling machine, Scissor bolter, HDPE pipe storm drainage system, POM D071 return thickener, LM17 probe, Iron bundle truck, Breaker tip, Jumbo drilling rig, Stilson key, Tire lever, Sledgehammer, Doosan RB equipment, Combination wrench, Hydraulic load maintenance equipment, Mechanized support scissor, Shotcrete gun, Three-way pear pipette, Nitrogen hose, Hydraulic fill pipe.\", \n",
    "        \"Drill rod, Jumbo, Sodium sulphide pump, Hand bar, Pulley motor, Oil cylinder, Scaffold, Truck, Bolt, Drilling machine, Stilson key, Steel wire rope, Winch, Jack, Suction valve, Cable pump, Locomotive, Electrical system, Shotcrete equipment, Ingot rotary table, Air lance, Concrete throwing team, Geho pump, Metal rake, Hydraulic hammer, Chisel, Lubricant, Strip set, Mining car, Diamond drill, Fisherman winch cable, Calibrator, Welding equipment, Peristaltic pump, Tirfor, Autoclave, Conveyor belt, Ventilation plug, Cleaning mechanism, Water hose, Manipulator, Stone cutting machine, Scoop lip, Hoist, Jackleg.\", \n",
    "        \"Split set intersection, Electric board 440V 400A, Thermomagnetic key, Panel shell, Mixkret, Autoclave, Anfo loader, Dumper, Pump, Automatic sampler, Platform, Wheel loader, Manual tick, Hydraulic fill, Intermediate cardan protector, Lamp, Motor transmission belt, Compressor, Cross cutter, Conveyor belt, Suction spool, Scrubber, Hydraulic cylinder, Vibrator, Scoop, Electric cable, PVC pipe, Fan belt, Key, Hose, Shotcrete.\"\n",
    "    ],\n",
    "    \"Инструменты и техника (пример на русском)\": [\n",
    "        \"Молоток, Лопата, Скобозабивной станок, Магнитометр.\", \n",
    "        \"Сварочное оборудование, Автомобиль, Насосы и воздуходувки, Инструменты для горных работ и инспекции, Силовые кабели и розетки.\", \n",
    "        \"Гидравлический цилиндр, Отбойный молоток, Радиально-сверлильный станок, Гидравлический насос, Тележка для перевозки рулонов железа.\", \n",
    "        \"Насос для перекачки сульфида натрия, Локомотив, Шкив двигателя, Поворотный стол для обработки слитков, Оборудование для торкретирования бетона, Камнерезный станок.\", \n",
    "        \"Электрическая панель 440V 400A, Горно-шахтная самоходная машина, Конвейерная лента, Ремень вентилятора, Самосвал-погрузчик.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Создаем DataFrame\n",
    "df_descr = pd.DataFrame(data, index=[\"1 уровень\", \"2 уровень\", \"3 уровень\", \"4 уровень\", \"5 уровень\"])\n",
    "\n",
    "# Настраиваем стили для выравнивания текста и заголовков\n",
    "styled_df = df_descr.style.set_properties(**{\n",
    "    'text-align': 'left',\n",
    "    'vertical-align': 'top'\n",
    "}).set_table_styles([\n",
    "    {'selector': 'th', 'props': [('text-align', 'center'), ('vertical-align', 'top')]},\n",
    "    {'selector': 'td', 'props': [('text-align', 'left'), ('vertical-align', 'top')]}\n",
    "])\n",
    "\n",
    "# Преобразуем стилизованный DataFrame в HTML и выводим в Jupyter Notebook\n",
    "html = styled_df.to_html()\n",
    "\n",
    "# Отображаем HTML\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27885022-0688-437b-b031-cb5244425c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка наличия строк с Potential Accident Level Encoded = 6\n",
    "count_6 = df[df['Potential Accident Level Encoded'] == 6].shape[0]\n",
    "print(f'Количество строк с Potential Accident Level Encoded = 6: {count_6}')\n",
    "\n",
    "# Удаление всех строк с Potential Accident Level Encoded = 6\n",
    "df = df[df['Potential Accident Level Encoded'] != 6]\n",
    "\n",
    "# Проверка количества строк после удаления\n",
    "print(f'Количество строк после удаления: {df.shape[0]}')\n",
    "\n",
    "# Проверка информации о DataFrame\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7341e0a",
   "metadata": {},
   "source": [
    "<h3>Создадим новые колонки и сохраним датасет.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7e3e79-5fb1-49c1-a9d3-50d758e500b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание новых колонок на основе значения \"Potential Accident Level Encoded\"\n",
    "def map_data(level):\n",
    "    return {\n",
    "        \"Positions\": data[\"Должности\"][level-1],\n",
    "        \"Actions\": data[\"Действия\"][level-1],\n",
    "        \"Tools_and_equipment\": data[\"Инструменты и техника\"][level-1]\n",
    "    }\n",
    "\n",
    "# Применяем функцию к каждому значению в колонке \"Potential Accident Level Encoded\"\n",
    "mapped_data = df['Potential Accident Level Encoded'].apply(map_data)\n",
    "\n",
    "# Преобразуем Series of dicts в DataFrame\n",
    "mapped_df = pd.DataFrame(mapped_data.tolist())\n",
    "\n",
    "# Объединяем оригинальный DataFrame с новым DataFrame\n",
    "result_df = pd.concat([df.reset_index(drop=True), mapped_df], axis=1)\n",
    "\n",
    "# Сохранение нового датасета в файл\n",
    "result_df.to_csv('df_pos_act_tools.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a11adaa-82c3-4b03-98e8-fe6fa593628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_pos_act_tools.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8241ec1d-3729-47a2-b062-4233438aeebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd1e6a-3281-44cc-80e8-e5ed6cb251d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "df_copy['Cleaned_Description'] = df_copy['Cleaned_Description'].apply(lambda x: (x[:47] + '...') if len(x) > 50 else x)\n",
    "df_copy['Positions'] = df_copy['Positions'].apply(lambda x: (x[:47] + '...') if len(x) > 50 else x)\n",
    "df_copy['Actions'] = df_copy['Actions'].apply(lambda x: (x[:47] + '...') if len(x) > 50 else x)\n",
    "df_copy['Tools_and_equipment'] = df_copy['Tools_and_equipment'].apply(lambda x: (x[:47] + '...') if len(x) > 50 else x)\n",
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8b49c-7db5-4504-99d4-e69b909ea9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применение One-Hot Encoding к категориальным столбцам\n",
    "df = pd.get_dummies(df, columns=['Genre', 'Employee or Third Party', 'Critical Risk', 'Month'])\n",
    "\n",
    "# Векторизация текста с использованием TF-IDF для Cleaned_Description, Positions, Actions, Tools_and_equipment\n",
    "tfidf = TfidfVectorizer(max_features=10000)\n",
    "\n",
    "# Векторизация для каждого текстового столбца и объединение результатов\n",
    "for column in ['Cleaned_Description', 'Positions', 'Actions', 'Tools_and_equipment']:\n",
    "    X_tfidf = tfidf.fit_transform(df[column])\n",
    "    X_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=[f\"{column}_{feature}\" for feature in tfidf.get_feature_names_out()])\n",
    "    df = pd.concat([df.drop(columns=[column]), X_tfidf_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88feb3d-b376-43f0-8367-d56c19287203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание копии DataFrame и сокращение текста\n",
    "df_copy = df.copy()\n",
    "\n",
    "# Пример сокращения текстов в копии DataFrame\n",
    "for column in ['Cleaned_Description', 'Positions', 'Actions', 'Tools_and_equipment']:\n",
    "    if column in df_copy.columns:\n",
    "        df_copy[column] = df_copy[column].apply(lambda x: (x[:47] + '...') if isinstance(x, str) and len(x) > 50 else x)\n",
    "\n",
    "df_copy.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0b2843-0855-47b3-880f-b4ba34cc84bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0484342d-1b54-4e4a-80f5-e7c734804e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделяем признаки и метки\n",
    "X = df.drop(columns=['Potential Accident Level Encoded'])\n",
    "y = df['Potential Accident Level Encoded']\n",
    "\n",
    "# Разбиение на тренировочную и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df94967",
   "metadata": {},
   "source": [
    "# Обучение. Градиентный бустинг. Вариант 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7b7400-93ae-443c-bba4-4956570f585d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение гиперпараметров для подбора\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# Настройка GridSearchCV\n",
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=gb_clf, param_grid=param_grid, cv=3, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "# Обучение модели с подбором гиперпараметров\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Лучшие параметры\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Оценка модели с лучшими параметрами\n",
    "best_gb_clf = grid_search.best_estimator_\n",
    "y_pred_gb = best_gb_clf.predict(X_test)\n",
    "\n",
    "gb_report = classification_report(y_test, y_pred_gb, target_names=[f\"{i+1}\" for i in class_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b58514",
   "metadata": {},
   "source": [
    "# Обучение. Нейронная сеть. Вариант 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef5f56-074c-42d9-a11f-8f6dc358f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование всех столбцов в числовой формат\n",
    "def convert_to_numeric(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'bool':\n",
    "            df[col] = df[col].astype(int)\n",
    "        elif df[col].dtype == 'object':\n",
    "            try:\n",
    "                df[col] = df[col].astype(float)\n",
    "            except ValueError:\n",
    "                df[col] = pd.factorize(df[col])[0]\n",
    "    return df\n",
    "\n",
    "# Выделяем признаки и метки\n",
    "X = df.drop(columns=['Potential Accident Level Encoded'])\n",
    "y = df['Potential Accident Level Encoded']\n",
    "\n",
    "# Преобразование всех данных в числовой формат\n",
    "X = convert_to_numeric(X)\n",
    "\n",
    "# Разбиение на тренировочную и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Преобразование меток классов в диапазон от 0 до num_classes-1\n",
    "y_train = y_train - 1\n",
    "y_test = y_test - 1\n",
    "\n",
    "# Преобразование данных в тензоры\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Создание DataLoader для PyTorch\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Определение модели нейронной сети\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)  # уменьшение размера скрытого слоя\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, num_classes)\n",
    "        self.dropout = nn.Dropout(0.6)  # увеличение dropout\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.batch_norm2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 128  # уменьшение скрытого слоя\n",
    "num_classes = len(y.unique())\n",
    "\n",
    "model = MLP(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Определение функции потерь и оптимизатора с L2-регуляризацией\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.01)  # уменьшение learning rate и увеличение weight decay\n",
    "\n",
    "# Обучение модели с ранним прекращением\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "best_test_loss = float('inf')\n",
    "patience = 10  # Число эпох без улучшений перед остановкой\n",
    "early_stopping_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "    # Проверка на тестовом наборе\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    test_losses.append(test_loss / len(test_loader))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}\")\n",
    "\n",
    "    # Раннее прекращение\n",
    "    if test_losses[-1] < best_test_loss:\n",
    "        best_test_loss = test_losses[-1]\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# Оценка модели\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.extend((predicted + 1).numpy())  # Сдвиг предсказанных значений на +1\n",
    "        y_true.extend((y_batch + 1).numpy())    # Сдвиг истинных значений на +1 для правильного сравнения\n",
    "\n",
    "# Проверка на переобучение и недообучение\n",
    "plt.plot(range(len(train_losses)), train_losses, label='Train Loss')\n",
    "plt.plot(range(len(test_losses)), test_losses, label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Train and Test Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df1d2b2",
   "metadata": {},
   "source": [
    "Сравним результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d4f357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка модели Gradient Boosting\n",
    "# gb_report = classification_report(y_test, y_pred_gb, target_names=[f\"{i+1}\" for i in class_labels])\n",
    "print(\"Gradient Boosting Classification Report:\")\n",
    "print(gb_report)\n",
    "\n",
    "# Отчет о классификации\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(\"Neural Network Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0efe97",
   "metadata": {},
   "source": [
    "Видим, что модели подстроились под данные, что соответственно нам не подходит."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f014138",
   "metadata": {},
   "source": [
    "# Предобработка датасета для обучения. Вариант 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bb3b65",
   "metadata": {},
   "source": [
    "<h3>Для каждой строчки с описанием происшествия определим:</h3> \n",
    "<br>1. Должность.\n",
    "<br>2. Действия, которые совершал работник.\n",
    "<br>3. Инструменты и техника.\n",
    "\n",
    "Добавим эти данные для каждой обработанной строки (создадим новые поля)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8141d1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('descr_cleaned_dataset.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfa20ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем словарь с данными для таблицы\n",
    "data = {\n",
    "    \"Должности\": [\n",
    "        \"Auxiliary assistant, Collaborator, Clerk, Geologist, Safety technical, Mining technician, Mechanical, Danilo Silva, Technician.\", \n",
    "        \"Forklift operator, Collaborator, Operator, Chief guard, Assistant, Engineer trainee, Teacher, Store attendant, Welder, Topographic surveyor, Operator bolter, Technician, Mixkret operator, Geologist, Maintenance team, Mechanic, Surveying worker, Industrial cleaning worker, Food preparer, Mobile equipment maintenance team, Comedor worker.\", \n",
    "        \"Collaborator, Mechanic technician, Electrician supervisor, Master loader, Equipment assistant, Official operator, Assistant loader, Master shipper, Truck crane operator, Mechanic, Technician, Civil operator, Bolted assistant, Welder, Helper, Boltec technician, Sampler, Electrician, Operator, Mechanic operator.\", \n",
    "        \"Maintenance Supervisor, Mechanic, Driller assistant, Operator, Mason Assistant, Bolter Operator, Mill Operator, Assistant Mechanic, Welder, Technician, Truck Driver, Plant Worker, Blaster, Maintenance worker.\", \n",
    "        \"Mixkret operator, Resident engineer, Filter operator, Autoclave operator, Mechanic, Pilot, Mixer operator, Wheel loader operator, Operator of the concrete plant, Scalar operator.\"\n",
    "    ],\n",
    "    \"Действия\": [\n",
    "        \"Unlock, Identify hexagonal head, Climb platform, Exert pressure hand, Try unclog material, Detach project, Enable place winch control room, Get short walk slip, Sit floor, Make contact, Carry sediment collection, Run meter, Evaluate geological point, Give access area, Know drainage point, Strike sting weed neck, Perform soil collection activity, Attach wire, Perform magnetometric GPS, Use sunglass, Carry refractory brick, Chop activity, Place support bus bar section, Execute soil sample task, Open machete, Break magnetometer Antenna, Move acquisition line, Traverse drainage.\", \n",
    "        \"Cleaning metal structures, Operating a crane and forklift, Using a hammer and chisel, Releasing a blade and manually displacing sheets, Operating a truck and adjusting bolts, Cutting electro welded mesh, Opening access areas with tools, Welding and inspecting mining cars, Handling chemicals, Performing maintenance on various equipment, Using a torch for cutting activities, Loading and unloading materials, Preparing geological maps and conducting surveys, Painting floors, Carrying out inspections, Preparing food, Cleaning industrial areas, Handling ventilation equipment.\", \n",
    "        \"Excavation work, Unload operation, Unclog discharge mouth, Maneuver to unhook hose, Turn pulley manually, Grab transmission belt, Verify belt tension, Install segment of polyurethane pulley, Clean shutter with air lance, Perform truck unload operation, Remove rope tie, Perform disconnection of power cable, Detach upper support point, Verify remaining position, Identify rock mesh, Change drill bit, Release coupling, Replace telescopic expansion joint, Position portable ladder, Hold base of loader, Clean spatula spear window boiler, Perform radial drilling, Activate hydraulic pump inspection cover, Install support mesh cloth, Handle water supply hose, Verify lock failure, Dismantle scaffold, Perform carbon steel pipe mark activity, Perform maintenance on motor support, Place protective plate on fuel tank, Perform supply operation for zinc powder container, Perform maintenance activity on transmission belt, Test soft starter engine belt, Perform mechanical support activity, Conduct inspection of sulfuric acid spill line, Carry out sand electrolysis piece, Perform brushcutter operation, Handle pneumatic conveyor, Transport dust zinc container, Lower metal sheet, Assemble activity for polypropylene pipe, Clean area near conveyor, Move locomotive personnel, Perform drilling activity with LM17 probe, Remove bucket of pulp sample, Supervise ustulation activity, Carry inspection cut block level OB6A.\", \n",
    "        \"Loosening support, Facilitating removal, Tightening, Activating pump, Designing area, Applying aid, Positioning pot, Operating drill, Cleaning position, Securing pipe, Lifting platform, Aligning cathode press, Manually moving steel cabinet, Drilling hole, Removing flange, Applying shotcrete, Preparing oil cylinder, Mounting rail platform, Feeding bag into furnace, Supporting stabilizer, Conducting maintenance, Checking work front, Verifying ventilation, Placing mesh, Pulling support mesh, Unloading ore, Reshaping hand, Unlocking rod, Tightening bolt, Evaluating acid leakage, Unloading residual water, Operating equipment, Driving truck, Inspecting equipment, Removing suction pipe, Cleaning low floor, Wearing safety equipment, Entering filter belt, Preparing construction, Perform rock untie, Lift brace, Hoisting and setting up equipment, Changing fuses, Soil collection, Cleaning area, Installation of ventilation plug, Welding steel plate, Unloading material, Operating overflow system, Manipulating hose, Testing equipment.\", \n",
    "        \"Open the electric board, Proceed with the installation, Remove the lock, Use the thermomagnetic key, Make phase contact, Check voltage, Plug socket, Cut wire, Transfer pump, Clean pump, Manipulate motor pump transmission, Change cable, Start equipment, Stop equipment, Perform sanitation, Clean compressor, Lubricate equipment, Load explosive equipment, Putty work, Clean material, Change conveyor belt.\"\n",
    "    ],\n",
    "    \"Инструменты и техника\": [\n",
    "        \"Metal piece, Shovel, Bucket, Canvas, Wooden handle, Metallic hammer, Sickle, Electric cable, Magnetometric GPS, Antenna, Hammer, Rock, Machete, Refractor, Shovel, Tabola, Scaller breaker arm, Hand tool, Structure equipment, Magnetometer, Screen.\", \n",
    "        \"Forklift, Hammer and chisel, Ladder, Manual displacement tools, Truck (A30), Electro welded mesh cutter, Machete, Welding equipment, Mixing equipment, Loaders (e.g., J005A), Sludge lever, Shears, Scissor bolter, Mona car, Laboratory sampling tools, Chemical containers, Pressure hoses, Pumps and blowers, Mining and inspection tools, High-pressure pump gun, Shovels and cleaning tools, Power cables and sockets, Cathode cranes, Zinc sheets, Wooden stumps, Rail tracks and corridors, Industrial cleaning equipment, Soil activity tools (e.g., pickaxe).\", \n",
    "        \"Ustilago powder, Silo truck, Transmission belt HM pump, Polyurethane pulley, Electro welded mesh, Hydraulic cylinder, Winch pulley, Air lance, Telescopic ladder, Metal bar hammer, Telescopic expansion joint HDPE pipe, Spatula spear window boiler, Simba M4C ITH equipment, Hydraulic pump, Volumetric balloon, Radial drilling machine, Scissor bolter, HDPE pipe storm drainage system, POM D071 return thickener, LM17 probe, Iron bundle truck, Breaker tip, Jumbo drilling rig, Stilson key, Tire lever, Sledgehammer, Doosan RB equipment, Combination wrench, Hydraulic load maintenance equipment, Mechanized support scissor, Shotcrete gun, Three-way pear pipette, Nitrogen hose, Hydraulic fill pipe.\", \n",
    "        \"Drill rod, Jumbo, Sodium sulphide pump, Hand bar, Pulley motor, Oil cylinder, Scaffold, Truck, Bolt, Drilling machine, Stilson key, Steel wire rope, Winch, Jack, Suction valve, Cable pump, Locomotive, Electrical system, Shotcrete equipment, Ingot rotary table, Air lance, Concrete throwing team, Geho pump, Metal rake, Hydraulic hammer, Chisel, Lubricant, Strip set, Mining car, Diamond drill, Fisherman winch cable, Calibrator, Welding equipment, Peristaltic pump, Tirfor, Autoclave, Conveyor belt, Ventilation plug, Cleaning mechanism, Water hose, Manipulator, Stone cutting machine, Scoop lip, Hoist, Jackleg.\", \n",
    "        \"Split set intersection, Electric board 440V 400A, Thermomagnetic key, Panel shell, Mixkret, Autoclave, Anfo loader, Dumper, Pump, Automatic sampler, Platform, Wheel loader, Manual tick, Hydraulic fill, Intermediate cardan protector, Lamp, Motor transmission belt, Compressor, Cross cutter, Conveyor belt, Suction spool, Scrubber, Hydraulic cylinder, Vibrator, Scoop, Electric cable, PVC pipe, Fan belt, Key, Hose, Shotcrete.\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Добавление новых колонок\n",
    "df['Positions'] = \"\"\n",
    "df['Actions'] = \"\"\n",
    "df['Tools_and_equipment'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d938e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка наличия строк с Potential Accident Level Encoded = 6\n",
    "count_6 = df[df['Potential Accident Level Encoded'] == 6].shape[0]\n",
    "print(f'Количество строк с Potential Accident Level Encoded = 6: {count_6}')\n",
    "\n",
    "# Удаление всех строк с Potential Accident Level Encoded = 6\n",
    "df = df[df['Potential Accident Level Encoded'] != 6]\n",
    "\n",
    "# Проверка количества строк после удаления\n",
    "print(f'Количество строк после удаления: {df.shape[0]}')\n",
    "\n",
    "# Проверка информации о DataFrame\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5271bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для поиска и заполнения данных из словаря по уровню происшествия без учета регистра\n",
    "def fill_columns_by_level(description, level, data):\n",
    "    positions = data[\"Должности\"][level-1]\n",
    "    actions = data[\"Действия\"][level-1]\n",
    "    tools_and_equipment = data[\"Инструменты и техника\"][level-1]\n",
    "    \n",
    "    description = description.lower()\n",
    "    \n",
    "    found_positions = [term for term in positions.split(\", \") if re.search(r'\\b' + re.escape(term.lower()) + r'\\b', description)]\n",
    "    found_actions = [term for term in actions.split(\", \") if re.search(r'\\b' + re.escape(term.lower()) + r'\\b', description)]\n",
    "    found_tools_and_equipment = [term for term in tools_and_equipment.split(\", \") if re.search(r'\\b' + re.escape(term.lower()) + r'\\b', description)]\n",
    "\n",
    "    return \", \".join(found_positions), \", \".join(found_actions), \", \".join(found_tools_and_equipment)\n",
    "\n",
    "# Применение функции к каждой строке\n",
    "for index, row in df.iterrows():\n",
    "    level = row['Potential Accident Level Encoded']\n",
    "    if level in [1, 2, 3, 4, 5]:\n",
    "        positions, actions, tools_and_equipment = fill_columns_by_level(row['Cleaned_Description'], level, data)\n",
    "        df.loc[index, 'Positions'] = positions\n",
    "        df.loc[index, 'Actions'] = actions\n",
    "        df.loc[index, 'Tools_and_equipment'] = tools_and_equipment\n",
    "\n",
    "# Сохранение нового датасета в файл\n",
    "df.to_csv('df_pos_act_tools.csv', index=False)\n",
    "\n",
    "print(\"Новый датасет сохранен в файл 'df_pos_act_tools.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0994a1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_pos_act_tools.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dd2071",
   "metadata": {},
   "source": [
    "Видим, что не все данные заполнились."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b41597e",
   "metadata": {},
   "source": [
    "<h3>Дозаполним пустые данные вручную, проанализировав каждое описание происшествия. Соохраним полученные данные в новый датасет df_pos_act_tools_manual_processing.csv</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ac5425",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_pos_act_tools_manual_processing_2.csv', sep=';')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a32a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Удаление колонки \"Cleaned_Description\"\n",
    "# df = df.drop(columns=['Cleaned_Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75305f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c9225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для приведения текста к нижнему регистру и удаления пунктуации\n",
    "def preprocess_text(text):\n",
    "    # Приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "    # Удаление пунктуации\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Применение функции к нужным колонкам\n",
    "columns_to_preprocess = ['Positions', 'Actions', 'Tools_and_equipment']\n",
    "for column in columns_to_preprocess:\n",
    "    df[column] = df[column].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54660609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка результата\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4af03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применение One-Hot Encoding к категориальным столбцам\n",
    "df = pd.get_dummies(df, columns=['Genre', 'Employee or Third Party', 'Critical Risk', 'Month'])\n",
    "\n",
    "# Векторизация текста с использованием TF-IDF для Cleaned_Description, Positions, Actions, Tools_and_equipment\n",
    "tfidf = TfidfVectorizer(max_features=10000)\n",
    "\n",
    "# Векторизация для каждого текстового столбца и объединение результатов\n",
    "# for column in ['Cleaned_Description', 'Positions', 'Actions', 'Tools_and_equipment']:\n",
    "for column in ['Cleaned_Description', 'Positions', 'Actions', 'Tools_and_equipment']:\n",
    "    X_tfidf = tfidf.fit_transform(df[column])\n",
    "    X_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=[f\"{column}_{feature}\" for feature in tfidf.get_feature_names_out()])\n",
    "    df = pd.concat([df.drop(columns=[column]), X_tfidf_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9748ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание копии DataFrame\n",
    "df_copy = df.copy()\n",
    "df_copy.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ced9d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделяем признаки и метки\n",
    "X = df.drop(columns=['Potential Accident Level Encoded'])\n",
    "y = df['Potential Accident Level Encoded']\n",
    "\n",
    "# Разбиение на тренировочную и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b1b51",
   "metadata": {},
   "source": [
    "# Обучение. Градиентный бустинг. Вариант 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d00f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение гиперпараметров для подбора\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# Настройка GridSearchCV\n",
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=gb_clf, param_grid=param_grid, cv=3, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "# Обучение модели с подбором гиперпараметров\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Лучшие параметры\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Оценка модели с лучшими параметрами\n",
    "best_gb_clf = grid_search.best_estimator_\n",
    "y_pred_gb = best_gb_clf.predict(X_test)\n",
    "\n",
    "gb_report = classification_report(y_test, y_pred_gb, target_names=[f\"{i+1}\" for i in class_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa5e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование всех столбцов в числовой формат\n",
    "def convert_to_numeric(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'bool':\n",
    "            df[col] = df[col].astype(int)\n",
    "        elif df[col].dtype == 'object':\n",
    "            try:\n",
    "                df[col] = df[col].astype(float)\n",
    "            except ValueError:\n",
    "                df[col] = pd.factorize(df[col])[0]\n",
    "    return df\n",
    "\n",
    "# Выделяем признаки и метки\n",
    "X = df.drop(columns=['Potential Accident Level Encoded'])\n",
    "y = df['Potential Accident Level Encoded']\n",
    "\n",
    "# Преобразование всех данных в числовой формат\n",
    "X = convert_to_numeric(X)\n",
    "\n",
    "# Разбиение на тренировочную и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Преобразование меток классов в диапазон от 0 до num_classes-1\n",
    "y_train = y_train - 1\n",
    "y_test = y_test - 1\n",
    "\n",
    "# Преобразование данных в тензоры\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Создание DataLoader для PyTorch\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Определение модели нейронной сети\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)  # уменьшение размера скрытого слоя\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, num_classes)\n",
    "        self.dropout = nn.Dropout(0.6)  # увеличение dropout\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.batch_norm2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 128  # уменьшение скрытого слоя\n",
    "num_classes = len(y.unique())\n",
    "\n",
    "model = MLP(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Определение функции потерь и оптимизатора с L2-регуляризацией\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.01)  # уменьшение learning rate и увеличение weight decay\n",
    "\n",
    "# Обучение модели с ранним прекращением\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "best_test_loss = float('inf')\n",
    "patience = 10  # Число эпох без улучшений перед остановкой\n",
    "early_stopping_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "    # Проверка на тестовом наборе\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    test_losses.append(test_loss / len(test_loader))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}\")\n",
    "\n",
    "    # Раннее прекращение\n",
    "    if test_losses[-1] < best_test_loss:\n",
    "        best_test_loss = test_losses[-1]\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# Оценка модели\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.extend((predicted + 1).numpy())  # Сдвиг предсказанных значений на +1\n",
    "        y_true.extend((y_batch + 1).numpy())    # Сдвиг истинных значений на +1 для правильного сравнения\n",
    "\n",
    "# Проверка на переобучение и недообучение\n",
    "plt.plot(range(len(train_losses)), train_losses, label='Train Loss')\n",
    "plt.plot(range(len(test_losses)), test_losses, label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Train and Test Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d49942",
   "metadata": {},
   "source": [
    "Сравним результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6378d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка модели Gradient Boosting\n",
    "# gb_report = classification_report(y_test, y_pred_gb, target_names=[f\"{i+1}\" for i in class_labels])\n",
    "print(\"Gradient Boosting Classification Report:\")\n",
    "print(gb_report)\n",
    "\n",
    "# Отчет о классификации\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(\"Neural Network Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6bde90",
   "metadata": {},
   "source": [
    "# Сравним итоговые результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c31b85",
   "metadata": {},
   "source": [
    "![SNOWFALL](comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f8df10",
   "metadata": {},
   "source": [
    "<b>Вывод:</b>\n",
    "\n",
    "<b>Анализ Gradient Boosting:</b> Вариант 3 демонстрирует улучшенные результаты по всем метрикам по сравнению с Вариантом 1. Увеличение точности, F1-оценки и значения Recall указывает на то, что Вариант 3 более эффективен в классификации и лучше справляется с задачей.\n",
    "\n",
    "<b>Анализ Neural Network:</b> Вариант 3 также демонстрирует улучшение по сравнению с Вариантом 1, однако разница в результатах между двумя вариантами меньшая, чем у Gradient Boosting. Тем не менее, Вариант 3 лучше в точности и Recall по сравнению с Вариантом 1.\n",
    "\n",
    "Видим, что итоговые результаты в обоих случах довольно слабые. Попробуем использовтаь более продвинутые модели, такие как <b>LightGBM</b>.\n",
    "\n",
    "Отдельно стоит отметить, что описание происшествий очень сильно разрозненные и практически не похожи друг на друга за некоторыми исключениями."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d991096",
   "metadata": {},
   "source": [
    "# Обучение модели LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e750ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_pos_act_tools_manual_processing_2.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f95270",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_preprocess = ['Positions', 'Actions', 'Tools_and_equipment']\n",
    "for column in columns_to_preprocess:\n",
    "    df[column] = df[column].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d599f0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применение One-Hot Encoding к категориальным столбцам\n",
    "df = pd.get_dummies(df, columns=['Genre', 'Employee or Third Party', 'Critical Risk', 'Month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cdc13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделяем признаки и метки\n",
    "X = df.drop(columns=['Potential Accident Level Encoded'])\n",
    "y = df['Potential Accident Level Encoded']\n",
    "\n",
    "# Разбиение на тренировочную и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da3e951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d1254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32893b46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a01409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc36e76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8581696e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb216aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4253250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815e1081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef14612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2068cc8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddea8b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e7539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d256099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Повторное заполнение NaN значений\n",
    "# X_train = X_train.fillna(\"\")\n",
    "# X_test = X_test.fillna(\"\")\n",
    "\n",
    "# # Проверка на наличие NaN перед векторизацией\n",
    "# print(\"Проверка на NaN в X_train перед векторизацией:\\n\", X_train[['Positions', 'Actions', 'Tools_and_equipment']].isna().sum())\n",
    "# print(\"Проверка на NaN в X_test перед векторизацией:\\n\", X_test[['Positions', 'Actions', 'Tools_and_equipment']].isna().sum())\n",
    "\n",
    "# print(f\"Размеры X_train до векторизации: {X_train.shape}\")\n",
    "# print(f\"Размеры y_train: {y_train.shape}\")\n",
    "\n",
    "# # Векторизация текста с использованием TF-IDF для текстовых столбцов\n",
    "# tfidf_vectorizers = {}\n",
    "# for column in ['Cleaned_Description', 'Positions', 'Actions', 'Tools_and_equipment']:\n",
    "#     tfidf = TfidfVectorizer(max_features=10000)\n",
    "#     X_tfidf_train = tfidf.fit_transform(X_train[column].astype(str))\n",
    "    \n",
    "#     # Преобразуем обучающие данные и проверяем размеры\n",
    "#     X_tfidf_train_df = pd.DataFrame(X_tfidf_train.toarray(), columns=[f\"{column}_{feature}\" for feature in tfidf.get_feature_names_out()])\n",
    "#     X_tfidf_train_df.index = X_train.index  # Присвоение правильных индексов\n",
    "    \n",
    "#     print(f\"Размеры X_tfidf_train для {column}: {X_tfidf_train_df.shape}\")\n",
    "    \n",
    "#     # Объединение с сохранением индексов\n",
    "#     X_train = pd.concat([X_train.drop(columns=[column]), X_tfidf_train_df], axis=1)\n",
    "    \n",
    "#     # Применяем ту же трансформацию к тестовым данным\n",
    "#     X_tfidf_test = tfidf.transform(X_test[column].astype(str))\n",
    "#     X_tfidf_test_df = pd.DataFrame(X_tfidf_test.toarray(), columns=[f\"{column}_{feature}\" for feature in tfidf.get_feature_names_out()])\n",
    "#     X_tfidf_test_df.index = X_test.index  # Присвоение правильных индексов\n",
    "    \n",
    "#     X_test = pd.concat([X_test.drop(columns=[column]), X_tfidf_test_df], axis=1)\n",
    "    \n",
    "#     # Сохраняем трансформер для дальнейшего использования\n",
    "#     tfidf_vectorizers[column] = tfidf\n",
    "\n",
    "# # Проверка на совпадение размеров после векторизации\n",
    "# print(f\"Размеры X_train после векторизации: {X_train.shape}\")\n",
    "# print(f\"Размеры y_train после векторизации: {y_train.shape}\")\n",
    "\n",
    "# # Синхронизация индексов, если размеры не совпадают\n",
    "# if len(X_train) != len(y_train):\n",
    "#     X_train, y_train = X_train.align(y_train, axis=0)\n",
    "#     print(f\"Размеры после синхронизации: X_train={X_train.shape}, y_train={y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956adffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь X_train и X_test готовы для обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606e5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Определение гиперпараметров для подбора\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200],\n",
    "#     'learning_rate': [0.01, 0.1],\n",
    "#     'max_depth': [3, 5],\n",
    "#     'subsample': [0.8, 1.0],\n",
    "#     'min_child_samples': [20, 50]\n",
    "# }\n",
    "\n",
    "# # Настройка GridSearchCV\n",
    "# lgb_clf = LGBMClassifier(random_state=42)\n",
    "# grid_search = GridSearchCV(estimator=lgb_clf, param_grid=param_grid, cv=3, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "# # Обучение модели с подбором гиперпараметров\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Лучшие параметры\n",
    "# best_params = grid_search.best_params_\n",
    "# print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# # Оценка модели с лучшими параметрами\n",
    "# best_lgb_clf = grid_search.best_estimator_\n",
    "# y_pred_lgb = best_lgb_clf.predict(X_test)\n",
    "\n",
    "# # Оценка модели LightGBM\n",
    "# class_labels = sorted(y.unique())\n",
    "# lgb_report = classification_report(y_test, y_pred_lgb, target_names=[f\"{i+1}\" for i in class_labels])\n",
    "\n",
    "# print(\"LightGBM Classification Report:\")\n",
    "# print(lgb_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7d6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd79756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35976f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697982ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from gensim.models import Word2Vec\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "388a206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(model, text, size):\n",
    "    \"\"\"\n",
    "    Функция для векторизации текста на основе модели Word2Vec.\n",
    "    text: текст в виде списка токенов\n",
    "    model: обученная модель Word2Vec\n",
    "    size: размер выходного вектора\n",
    "    \"\"\"\n",
    "    vector = np.zeros(size)\n",
    "    count = 0\n",
    "    for word in text:\n",
    "        if word in model.wv:\n",
    "            vector += model.wv[word]\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        vector /= count\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fc0792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Разбиение на тренировочную и тестовую выборки\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# # Заполнение NaN значений\n",
    "# X_train = X_train.fillna(\"\")\n",
    "# X_test = X_test.fillna(\"\")\n",
    "\n",
    "# # Токенизация текста\n",
    "# X_train_tokens = X_train[['Cleaned_Description', 'Positions', 'Actions', 'Tools_and_equipment']].applymap(lambda x: x.split())\n",
    "# X_test_tokens = X_test[['Cleaned_Description', 'Positions', 'Actions', 'Tools_and_equipment']].applymap(lambda x: x.split())\n",
    "\n",
    "# # Обучение модели Word2Vec на токенизированных данных\n",
    "# word2vec_model = Word2Vec(sentences=pd.concat([X_train_tokens[col] for col in X_train_tokens.columns]).values.flatten(),\n",
    "#                           vector_size=100,  # Размер вектора\n",
    "#                           window=5,\n",
    "#                           min_count=1,\n",
    "#                           workers=4)\n",
    "\n",
    "# print(f\"Размеры X_train до векторизации: {X_train.shape}\")\n",
    "# print(f\"Размеры y_train до векторизации: {y_train.shape}\")\n",
    "\n",
    "# # Векторизация текста\n",
    "# for column in ['Cleaned_Description', 'Positions', 'Actions', 'Tools_and_equipment']:\n",
    "#     size = word2vec_model.vector_size\n",
    "#     X_train_vectors = np.array([vectorize_text(word2vec_model, tokens, size) for tokens in X_train_tokens[column]])\n",
    "#     X_test_vectors = np.array([vectorize_text(word2vec_model, tokens, size) for tokens in X_test_tokens[column]])\n",
    "    \n",
    "#     # Преобразуем в датафреймы\n",
    "#     X_train_vectors_df = pd.DataFrame(X_train_vectors, columns=[f\"{column}_w2v_{i}\" for i in range(size)])\n",
    "#     X_test_vectors_df = pd.DataFrame(X_test_vectors, columns=[f\"{column}_w2v_{i}\" for i in range(size)])\n",
    "    \n",
    "#     # Синхронизация индексов\n",
    "#     X_train_vectors_df.index = X_train.index\n",
    "#     X_test_vectors_df.index = X_test.index\n",
    "    \n",
    "#     # Объединение с основными данными\n",
    "#     X_train = pd.concat([X_train.drop(columns=[column]), X_train_vectors_df], axis=1)\n",
    "#     X_test = pd.concat([X_test.drop(columns=[column]), X_test_vectors_df], axis=1)\n",
    "\n",
    "# # Проверка на совпадение размеров после векторизации\n",
    "# print(f\"Размеры X_train после векторизации: {X_train.shape}\")\n",
    "# print(f\"Размеры y_train после векторизации: {y_train.shape}\")\n",
    "\n",
    "# # Синхронизация индексов, если размеры не совпадают\n",
    "# if len(X_train) != len(y_train):\n",
    "#     X_train, y_train = X_train.align(y_train, axis=0)\n",
    "#     print(f\"Размеры после синхронизации: X_train={X_train.shape}, y_train={y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7226d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Теперь X_train и X_test готовы для обучения модели\n",
    "\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Определение гиперпараметров для подбора\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200],\n",
    "#     'learning_rate': [0.01, 0.1],\n",
    "#     'max_depth': [3, 5],\n",
    "#     'subsample': [0.8, 1.0],\n",
    "#     'min_child_samples': [20, 50]\n",
    "# }\n",
    "\n",
    "# # Настройка GridSearchCV\n",
    "# lgb_clf = LGBMClassifier(random_state=42)\n",
    "# grid_search = GridSearchCV(estimator=lgb_clf, param_grid=param_grid, cv=3, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "# # Обучение модели с подбором гиперпараметров\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Лучшие параметры\n",
    "# best_params = grid_search.best_params_\n",
    "# print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# # Оценка модели с лучшими параметрами\n",
    "# best_lgb_clf = grid_search.best_estimator()\n",
    "# y_pred_lgb = best_lgb_clf.predict(X_test)\n",
    "\n",
    "# # Оценка модели LightGBM\n",
    "# class_labels = sorted(y.unique())\n",
    "# lgb_report = classification_report(y_test, y_pred_lgb, target_names=[f\"{i+1}\" for i in class_labels])\n",
    "\n",
    "# print(\"LightGBM Classification Report:\")\n",
    "# print(lgb_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f861c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train.dtypes)\n",
    "# print(X_test.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4774d",
   "metadata": {},
   "source": [
    "# CatBoost Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04d23d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_pos_act_tools_manual_processing_2.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28d75221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для приведения текста к нижнему регистру и удаления пунктуации\n",
    "def preprocess_text(text):\n",
    "    # Приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "    # Удаление пунктуации\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "columns_to_preprocess = ['Positions', 'Actions', 'Tools_and_equipment']\n",
    "for column in columns_to_preprocess:\n",
    "    df[column] = df[column].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b67119",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5610cb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AADementev\\AppData\\Local\\Temp\\ipykernel_3712\\1554581675.py:31: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  X_train_tokens = X_train[['Cleaned_Description', 'Positions', 'Actions', 'Tools_and_equipment']].applymap(lambda x: x.split())\n",
      "C:\\Users\\AADementev\\AppData\\Local\\Temp\\ipykernel_3712\\1554581675.py:32: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  X_test_tokens = X_test[['Cleaned_Description', 'Positions', 'Actions', 'Tools_and_equipment']].applymap(lambda x: x.split())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры X_train до векторизации: (339, 8)\n",
      "Размеры y_train до векторизации: (339,)\n",
      "Размеры X_train после векторизации: (339, 404)\n",
      "Размеры y_train после векторизации: (339,)\n",
      "Best parameters: {'depth': 4, 'iterations': 300, 'learning_rate': 0.1}\n",
      "CatBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.86      0.60      0.71        10\n",
      "           3       0.36      0.47      0.41        19\n",
      "           4       0.19      0.14      0.16        21\n",
      "           5       0.36      0.45      0.40        29\n",
      "           6       1.00      0.17      0.29         6\n",
      "\n",
      "    accuracy                           0.38        85\n",
      "   macro avg       0.55      0.37      0.39        85\n",
      "weighted avg       0.42      0.38      0.37        85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Загрузка данных\n",
    "df = pd.read_csv('df_pos_act_tools_manual_processing_2.csv', sep=';')\n",
    "\n",
    "# Предобработка текста\n",
    "columns_to_preprocess = ['Positions', 'Actions', 'Tools_and_equipment']\n",
    "for column in columns_to_preprocess:\n",
    "    df[column] = df[column].apply(preprocess_text)\n",
    "\n",
    "# Определение категориальных признаков\n",
    "categorical_features = ['Genre', 'Employee or Third Party', 'Critical Risk', 'Month']\n",
    "\n",
    "# Выделяем признаки и метки\n",
    "X = df.drop(columns=['Potential Accident Level Encoded'])\n",
    "y = df['Potential Accident Level Encoded']\n",
    "\n",
    "# Разбиение на тренировочную и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Заполнение NaN значений\n",
    "X_train = X_train.fillna(\"\")\n",
    "X_test = X_test.fillna(\"\")\n",
    "\n",
    "# Токенизация текста\n",
    "X_train_tokens = X_train[['Cleaned_Description', 'Positions', 'Actions', 'Tools_and_equipment']].applymap(lambda x: x.split())\n",
    "X_test_tokens = X_test[['Cleaned_Description', 'Positions', 'Actions', 'Tools_and_equipment']].applymap(lambda x: x.split())\n",
    "\n",
    "# Обучение модели Word2Vec на токенизированных данных\n",
    "word2vec_model = Word2Vec(sentences=pd.concat([X_train_tokens[col] for col in X_train_tokens.columns]).values.flatten(),\n",
    "                          vector_size=100,  # Размер вектора\n",
    "                          window=5,\n",
    "                          min_count=1,\n",
    "                          workers=4)\n",
    "\n",
    "print(f\"Размеры X_train до векторизации: {X_train.shape}\")\n",
    "print(f\"Размеры y_train до векторизации: {y_train.shape}\")\n",
    "\n",
    "# Векторизация текста\n",
    "for column in ['Cleaned_Description', 'Positions', 'Actions', 'Tools_and_equipment']:\n",
    "    size = word2vec_model.vector_size\n",
    "    X_train_vectors = np.array([vectorize_text(word2vec_model, tokens, size) for tokens in X_train_tokens[column]])\n",
    "    X_test_vectors = np.array([vectorize_text(word2vec_model, tokens, size) for tokens in X_test_tokens[column]])\n",
    "    \n",
    "    # Преобразуем в датафреймы\n",
    "    X_train_vectors_df = pd.DataFrame(X_train_vectors, columns=[f\"{column}_w2v_{i}\" for i in range(size)])\n",
    "    X_test_vectors_df = pd.DataFrame(X_test_vectors, columns=[f\"{column}_w2v_{i}\" for i in range(size)])\n",
    "    \n",
    "    # Синхронизация индексов\n",
    "    X_train_vectors_df.index = X_train.index\n",
    "    X_test_vectors_df.index = X_test.index\n",
    "    \n",
    "    # Объединение с основными данными\n",
    "    X_train = pd.concat([X_train.drop(columns=[column]), X_train_vectors_df], axis=1)\n",
    "    X_test = pd.concat([X_test.drop(columns=[column]), X_test_vectors_df], axis=1)\n",
    "\n",
    "# Проверка на совпадение размеров после векторизации\n",
    "print(f\"Размеры X_train после векторизации: {X_train.shape}\")\n",
    "print(f\"Размеры y_train после векторизации: {y_train.shape}\")\n",
    "\n",
    "# Синхронизация индексов, если размеры не совпадают\n",
    "if len(X_train) != len(y_train):\n",
    "    X_train, y_train = X_train.align(y_train, axis=0)\n",
    "    print(f\"Размеры после синхронизации: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "\n",
    "# Теперь X_train и X_test готовы для обучения модели\n",
    "\n",
    "# Определение гиперпараметров для подбора\n",
    "# param_grid = {\n",
    "#     'iterations': [100, 200],\n",
    "#     'learning_rate': [0.01, 0.1],\n",
    "#     'depth': [3, 5],\n",
    "#     'min_data_in_leaf': [20, 50]\n",
    "# }\n",
    "\n",
    "# param_grid = {\n",
    "#     'min_data_in_leaf': [10, 20, 50],\n",
    "#     'bootstrap_type': ['Bernoulli', 'Poisson']\n",
    "# }\n",
    "\n",
    "param_grid = {\n",
    "    'iterations': [300],\n",
    "    'depth': [4],\n",
    "    'learning_rate': [0.1],\n",
    "}\n",
    "\n",
    "# Настройка GridSearchCV для CatBoostClassifier\n",
    "cat_clf = CatBoostClassifier(cat_features=categorical_features, random_state=42, verbose=0)\n",
    "grid_search = GridSearchCV(estimator=cat_clf, param_grid=param_grid, cv=3, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "# Обучение модели с подбором гиперпараметров\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Лучшие параметры\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Оценка модели с лучшими параметрами\n",
    "best_cat_clf = grid_search.best_estimator_\n",
    "y_pred_cat = best_cat_clf.predict(X_test)\n",
    "\n",
    "# Оценка модели CatBoost\n",
    "class_labels = sorted(y.unique())\n",
    "cat_report = classification_report(y_test, y_pred_cat, target_names=[f\"{i+1}\" for i in class_labels])\n",
    "\n",
    "print(\"CatBoost Classification Report:\")\n",
    "print(cat_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf08355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d25fe0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c636b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ed90ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d73599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51275271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2b50ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29315673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265e61d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6fe922a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'depth': 4, 'iterations': 300, 'l2_leaf_reg': 5, 'learning_rate': 0.1}\n",
      "CatBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      1.00      0.80         4\n",
      "           2       0.75      0.21      0.32        29\n",
      "           3       0.30      0.36      0.33        22\n",
      "           4       0.32      0.61      0.42        23\n",
      "           5       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.38        85\n",
      "   macro avg       0.41      0.44      0.37        85\n",
      "weighted avg       0.45      0.38      0.35        85\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AADementev\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\AADementev\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\AADementev\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from gensim.models import Word2Vec\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. Загрузка данных\n",
    "# df = pd.read_csv('df_pos_act_tools_manual_processing_2.csv', sep=';')\n",
    "\n",
    "# 2. Преобразование категориальных признаков с помощью One-Hot Encoding\n",
    "categorical_columns = ['Genre', 'Employee or Third Party', 'Critical Risk', 'Month']\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "categorical_encoded = encoder.fit_transform(df[categorical_columns])\n",
    "\n",
    "# Преобразование в DataFrame\n",
    "categorical_encoded_df = pd.DataFrame(categorical_encoded, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "# 3. Преобразование текстовых признаков с помощью Word2Vec\n",
    "text_columns = ['Positions', 'Actions', 'Tools_and_equipment', 'Cleaned_Description']\n",
    "\n",
    "# Обучение модели Word2Vec на всех текстовых данных\n",
    "word2vec_models = {}\n",
    "text_vectors = []\n",
    "\n",
    "for col in text_columns:\n",
    "    text_data = df[col].apply(lambda x: x.split())\n",
    "    model = Word2Vec(sentences=text_data, vector_size=100, window=5, min_count=1, workers=4, sg=0)\n",
    "    word2vec_models[col] = model\n",
    "\n",
    "    # Среднее по вектору для каждого текста\n",
    "    vectors = text_data.apply(lambda words: np.mean([model.wv[word] for word in words if word in model.wv] or [np.zeros(100)], axis=0))\n",
    "    text_vectors.append(np.array(vectors.tolist()))\n",
    "\n",
    "# Преобразование в DataFrame\n",
    "text_encoded_df = pd.DataFrame(np.hstack(text_vectors))\n",
    "\n",
    "# 4. Объединение всех признаков\n",
    "X = pd.concat([categorical_encoded_df, text_encoded_df], axis=1)\n",
    "y = df['Potential Accident Level Encoded']\n",
    "\n",
    "# 5. Разделение на тренировочную и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 6. Обучение модели CatBoostClassifier с подбором гиперпараметров\n",
    "catboost_model = CatBoostClassifier(verbose=0, random_state=42)\n",
    "\n",
    "# Грид поиска гиперпараметров\n",
    "# param_grid = {\n",
    "#     'iterations': [100, 200, 300],\n",
    "#     'depth': [4, 6, 8],\n",
    "#     'learning_rate': [0.01, 0.05, 0.1],\n",
    "#     'l2_leaf_reg': [1, 3, 5]\n",
    "# }\n",
    "\n",
    "# Грид поиска гиперпараметров\n",
    "param_grid = {\n",
    "    'iterations': [300],\n",
    "    'depth': [4],\n",
    "    'learning_rate': [0.1],\n",
    "    'l2_leaf_reg': [5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=catboost_model, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Обучение модели\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Лучшие параметры\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Оценка модели с лучшими параметрами\n",
    "best_cat_clf = grid_search.best_estimator_\n",
    "y_pred_cat = best_cat_clf.predict(X_test)\n",
    "\n",
    "# Оценка модели CatBoost\n",
    "class_labels = sorted(y.unique())\n",
    "cat_report = classification_report(y_test, y_pred_cat, target_names=[f\"{i}\" for i in class_labels])\n",
    "\n",
    "print(\"CatBoost Classification Report:\")\n",
    "print(cat_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694e675f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33e892a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b88e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420345e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4c4ea39",
   "metadata": {},
   "source": [
    "<b>P.S. Тестовый запуск.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e86285-4506-436c-9aca-67f1c86d9c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_employee = {\n",
    "    # 'Name': ['Ivanov Ivan Ivanovich'],\n",
    "    'Genre': ['Male'],\n",
    "    'Employee or Third Party': ['Employee'],\n",
    "    'Critical Risk': ['Power lock'], # 5 уровень\n",
    "    'Positions': ['Assistant'], # 1,2 уровень\n",
    "    'Actions': ['Identify hexagonal head.'], # 1 уровень\n",
    "    'Tools_and_equipment': ['Hydraulic pump.'], # 3 уровень\n",
    "    'Cleaned_Description': ['Make sure that the equipment is fully ready to install the 4 detachable kits. Make sure that the operator is ready to supply power to your equipment. Remove the lock and open the electrical panel designed for 220 V and 200 A. Check that all tools and protective equipment are in good condition and ready for use. Lift the thermomagnetic wrench with extreme care. Please note that when lifting the thermomagnetic key, phase contact with the ground on the panel may occur. In case of contact of the phase with the ground on the panel, a flash may occur that can reach the operator. Take all necessary safety measures to prevent injury and damage. Use appropriate personal protective equipment (PPE).']  # 5 уровень\n",
    "}\n",
    "\n",
    "df_employee = pd.DataFrame(data_employee)\n",
    "\n",
    "# Применение One-Hot Encoding к категориальным столбцам\n",
    "df_employee = pd.get_dummies(df_employee, columns=['Genre', 'Employee or Third Party', 'Critical Risk'])\n",
    "\n",
    "# Применение TF-IDF к текстовым столбцам\n",
    "tfidf_dfs = []\n",
    "for column in ['Cleaned_Description', 'Positions', 'Actions', 'Tools_and_equipment']:\n",
    "    X_tfidf = tfidf.transform(df_employee[column])\n",
    "    tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=[f\"{column}_{feature}\" for feature in tfidf.get_feature_names_out()])\n",
    "    tfidf_dfs.append(tfidf_df)\n",
    "\n",
    "# Объединение TF-IDF столбцов и удаление исходных текстовых столбцов\n",
    "df_employee = pd.concat([df_employee.drop(columns=['Cleaned_Description', 'Positions', 'Actions', 'Tools_and_equipment'])] + tfidf_dfs, axis=1)\n",
    "\n",
    "# Преобразование всех данных в числовой формат\n",
    "df_employee = convert_to_numeric(df_employee)\n",
    "\n",
    "# Заполнение отсутствующих столбцов нулями и упорядочивание столбцов согласно обучающему набору данных\n",
    "missing_columns = list(set(X.columns) - set(df_employee.columns))\n",
    "missing_df = pd.DataFrame(0, index=df_employee.index, columns=missing_columns)\n",
    "df_employee = pd.concat([df_employee, missing_df], axis=1)\n",
    "\n",
    "# Упорядочивание столбцов\n",
    "df_employee = df_employee[X.columns]\n",
    "\n",
    "# Преобразование данных в тензоры\n",
    "X_employee_tensor = torch.tensor(df_employee.values, dtype=torch.float32)\n",
    "\n",
    "# Прогноз\n",
    "with torch.no_grad():\n",
    "    output = model(X_employee_tensor)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    predicted_class = predicted.item() + 1  # Сдвиг на +1, чтобы вернуть исходные классы\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccdd631",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
